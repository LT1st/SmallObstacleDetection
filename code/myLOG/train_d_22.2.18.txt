C:\ProgramData\Anaconda3\envs\t\python.exe D:/00code/small_obstacle_discovery-master/train.py
using depth? : True
u are running in : Windows
u are running in : Windows
DeepLab constructor: True
backbone constructor: True
DRN constr True
Using poly LR Scheduler!
开始模拟输入维度:torch.Size([2, 4, 512, 512])
C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
graph(%self.1 : __torch__.modeling.deeplab.DeepLab,
      %input.1 : Float(2:1048576, 4:262144, 512:512, 512:1, requires_grad=0, device=cpu)):
  %3480 : __torch__.modeling.decoder.Decoder = prim::GetAttr[name="decoder"](%self.1)
  %3446 : __torch__.modeling.aspp.ASPP = prim::GetAttr[name="aspp"](%self.1)
  %3390 : __torch__.modeling.backbone.drn.DRN = prim::GetAttr[name="backbone"](%self.1)
  %3732 : int = prim::Constant[value=4](), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3733 : int = prim::Constant[value=2](), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3734 : bool = prim::Constant[value=1](), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3735 : int = prim::Constant[value=0](), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3736 : bool = prim::Constant[value=0](), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3737 : int = prim::Constant[value=3](), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3738 : int = prim::Constant[value=1](), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3739 : None = prim::Constant(), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0
  %3740 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %3741 : float = prim::Constant[value=0.10000000000000001](), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %3742 : __torch__.torch.nn.modules.container.___torch_mangle_158.Sequential = prim::GetAttr[name="layer8"](%3390)
  %3743 : __torch__.torch.nn.modules.container.___torch_mangle_154.Sequential = prim::GetAttr[name="layer7"](%3390)
  %3744 : __torch__.torch.nn.modules.container.___torch_mangle_150.Sequential = prim::GetAttr[name="layer6"](%3390)
  %3745 : __torch__.torch.nn.modules.container.___torch_mangle_122.Sequential = prim::GetAttr[name="layer5"](%3390)
  %3746 : __torch__.torch.nn.modules.container.___torch_mangle_70.Sequential = prim::GetAttr[name="layer4"](%3390)
  %3747 : __torch__.torch.nn.modules.container.___torch_mangle_34.Sequential = prim::GetAttr[name="layer3"](%3390)
  %3748 : __torch__.torch.nn.modules.container.___torch_mangle_7.Sequential = prim::GetAttr[name="layer2"](%3390)
  %3749 : __torch__.torch.nn.modules.container.___torch_mangle_3.Sequential = prim::GetAttr[name="layer1"](%3390)
  %3750 : __torch__.torch.nn.modules.container.Sequential = prim::GetAttr[name="layer0"](%3390)
  %3751 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="1"](%3750)
  %3752 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name="0"](%3750)
  %3753 : Tensor = prim::GetAttr[name="weight"](%3752)
  %3754 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0
  %3755 : int[] = prim::ListConstruct(%3737, %3737), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0
  %3756 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0
  %3757 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0
  %input.2 : Float(2:4194304, 16:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::_convolution(%input.1, %3753, %3739, %3754, %3755, %3756, %3736, %3757, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3759 : Tensor = prim::GetAttr[name="running_var"](%3751)
  %3760 : Tensor = prim::GetAttr[name="running_mean"](%3751)
  %3761 : Tensor = prim::GetAttr[name="bias"](%3751)
  %3762 : Tensor = prim::GetAttr[name="weight"](%3751)
  %input.3 : Float(2:4194304, 16:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.2, %3762, %3761, %3760, %3759, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.4 : Float(2:4194304, 16:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::relu_(%input.3), scope: __module.backbone/__module.backbone.layer0/__module.backbone.layer0.2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3765 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_1.BatchNorm2d = prim::GetAttr[name="1"](%3749)
  %3766 : __torch__.torch.nn.modules.conv.___torch_mangle_0.Conv2d = prim::GetAttr[name="0"](%3749)
  %3767 : Tensor = prim::GetAttr[name="weight"](%3766)
  %3768 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer1/__module.backbone.layer1.0
  %3769 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer1/__module.backbone.layer1.0
  %3770 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer1/__module.backbone.layer1.0
  %3771 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer1/__module.backbone.layer1.0
  %input.5 : Float(2:4194304, 16:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::_convolution(%input.4, %3767, %3739, %3768, %3769, %3770, %3736, %3771, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer1/__module.backbone.layer1.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3773 : Tensor = prim::GetAttr[name="running_var"](%3765)
  %3774 : Tensor = prim::GetAttr[name="running_mean"](%3765)
  %3775 : Tensor = prim::GetAttr[name="bias"](%3765)
  %3776 : Tensor = prim::GetAttr[name="weight"](%3765)
  %input.6 : Float(2:4194304, 16:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.5, %3776, %3775, %3774, %3773, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer1/__module.backbone.layer1.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.7 : Float(2:4194304, 16:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::relu_(%input.6), scope: __module.backbone/__module.backbone.layer1/__module.backbone.layer1.2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3779 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_5.BatchNorm2d = prim::GetAttr[name="1"](%3748)
  %3780 : __torch__.torch.nn.modules.conv.___torch_mangle_4.Conv2d = prim::GetAttr[name="0"](%3748)
  %3781 : Tensor = prim::GetAttr[name="weight"](%3780)
  %3782 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.0
  %3783 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.0
  %3784 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.0
  %3785 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.0
  %input.8 : Float(2:2097152, 32:65536, 256:256, 256:1, requires_grad=1, device=cpu) = aten::_convolution(%input.7, %3781, %3739, %3782, %3783, %3784, %3736, %3785, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3787 : Tensor = prim::GetAttr[name="running_var"](%3779)
  %3788 : Tensor = prim::GetAttr[name="running_mean"](%3779)
  %3789 : Tensor = prim::GetAttr[name="bias"](%3779)
  %3790 : Tensor = prim::GetAttr[name="weight"](%3779)
  %input.9 : Float(2:2097152, 32:65536, 256:256, 256:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.8, %3790, %3789, %3788, %3787, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.10 : Float(2:2097152, 32:65536, 256:256, 256:1, requires_grad=1, device=cpu) = aten::relu_(%input.9), scope: __module.backbone/__module.backbone.layer2/__module.backbone.layer2.2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3793 : __torch__.modeling.backbone.drn.___torch_mangle_33.Bottleneck = prim::GetAttr[name="2"](%3747)
  %3794 : __torch__.modeling.backbone.drn.___torch_mangle_25.Bottleneck = prim::GetAttr[name="1"](%3747)
  %3795 : __torch__.modeling.backbone.drn.Bottleneck = prim::GetAttr[name="0"](%3747)
  %3796 : __torch__.torch.nn.modules.container.___torch_mangle_17.Sequential = prim::GetAttr[name="downsample"](%3795)
  %3797 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_13.BatchNorm2d = prim::GetAttr[name="bn3"](%3795)
  %3798 : __torch__.torch.nn.modules.conv.___torch_mangle_12.Conv2d = prim::GetAttr[name="conv3"](%3795)
  %3799 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_11.BatchNorm2d = prim::GetAttr[name="bn2"](%3795)
  %3800 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%3795)
  %3801 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_9.BatchNorm2d = prim::GetAttr[name="bn1"](%3795)
  %3802 : __torch__.torch.nn.modules.conv.___torch_mangle_8.Conv2d = prim::GetAttr[name="conv1"](%3795)
  %3803 : Tensor = prim::GetAttr[name="weight"](%3802)
  %3804 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv1
  %3805 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv1
  %3806 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv1
  %3807 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv1
  %input.11 : Float(2:4194304, 64:65536, 256:256, 256:1, requires_grad=1, device=cpu) = aten::_convolution(%input.10, %3803, %3739, %3804, %3805, %3806, %3736, %3807, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3809 : Tensor = prim::GetAttr[name="running_var"](%3801)
  %3810 : Tensor = prim::GetAttr[name="running_mean"](%3801)
  %3811 : Tensor = prim::GetAttr[name="bias"](%3801)
  %3812 : Tensor = prim::GetAttr[name="weight"](%3801)
  %input.12 : Float(2:4194304, 64:65536, 256:256, 256:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.11, %3812, %3811, %3810, %3809, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.13 : Float(2:4194304, 64:65536, 256:256, 256:1, requires_grad=1, device=cpu) = aten::relu_(%input.12), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3815 : Tensor = prim::GetAttr[name="weight"](%3800)
  %3816 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv2
  %3817 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv2
  %3818 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv2
  %3819 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv2
  %input.14 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.13, %3815, %3739, %3816, %3817, %3818, %3736, %3819, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3821 : Tensor = prim::GetAttr[name="running_var"](%3799)
  %3822 : Tensor = prim::GetAttr[name="running_mean"](%3799)
  %3823 : Tensor = prim::GetAttr[name="bias"](%3799)
  %3824 : Tensor = prim::GetAttr[name="weight"](%3799)
  %input.15 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.14, %3824, %3823, %3822, %3821, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.16 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.15), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3827 : Tensor = prim::GetAttr[name="weight"](%3798)
  %3828 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv3
  %3829 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv3
  %3830 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv3
  %3831 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv3
  %input.17 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.16, %3827, %3739, %3828, %3829, %3830, %3736, %3831, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3833 : Tensor = prim::GetAttr[name="running_var"](%3797)
  %3834 : Tensor = prim::GetAttr[name="running_mean"](%3797)
  %3835 : Tensor = prim::GetAttr[name="bias"](%3797)
  %3836 : Tensor = prim::GetAttr[name="weight"](%3797)
  %out.1 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.17, %3836, %3835, %3834, %3833, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %3838 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_16.BatchNorm2d = prim::GetAttr[name="1"](%3796)
  %3839 : __torch__.torch.nn.modules.conv.___torch_mangle_15.Conv2d = prim::GetAttr[name="0"](%3796)
  %3840 : Tensor = prim::GetAttr[name="weight"](%3839)
  %3841 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.downsample/__module.backbone.layer3.0.downsample.0
  %3842 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.downsample/__module.backbone.layer3.0.downsample.0
  %3843 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.downsample/__module.backbone.layer3.0.downsample.0
  %3844 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.downsample/__module.backbone.layer3.0.downsample.0
  %input.18 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.10, %3840, %3739, %3841, %3842, %3843, %3736, %3844, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.downsample/__module.backbone.layer3.0.downsample.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3846 : Tensor = prim::GetAttr[name="running_var"](%3838)
  %3847 : Tensor = prim::GetAttr[name="running_mean"](%3838)
  %3848 : Tensor = prim::GetAttr[name="bias"](%3838)
  %3849 : Tensor = prim::GetAttr[name="weight"](%3838)
  %residual.1 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.18, %3849, %3848, %3847, %3846, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.downsample/__module.backbone.layer3.0.downsample.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.19 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::add_(%out.1, %residual.1, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.20 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.19), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.0/__module.backbone.layer3.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3853 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_23.BatchNorm2d = prim::GetAttr[name="bn3"](%3794)
  %3854 : __torch__.torch.nn.modules.conv.___torch_mangle_22.Conv2d = prim::GetAttr[name="conv3"](%3794)
  %3855 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%3794)
  %3856 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv2"](%3794)
  %3857 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_19.BatchNorm2d = prim::GetAttr[name="bn1"](%3794)
  %3858 : __torch__.torch.nn.modules.conv.___torch_mangle_18.Conv2d = prim::GetAttr[name="conv1"](%3794)
  %3859 : Tensor = prim::GetAttr[name="weight"](%3858)
  %3860 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv1
  %3861 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv1
  %3862 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv1
  %3863 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv1
  %input.21 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.20, %3859, %3739, %3860, %3861, %3862, %3736, %3863, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3865 : Tensor = prim::GetAttr[name="running_var"](%3857)
  %3866 : Tensor = prim::GetAttr[name="running_mean"](%3857)
  %3867 : Tensor = prim::GetAttr[name="bias"](%3857)
  %3868 : Tensor = prim::GetAttr[name="weight"](%3857)
  %input.22 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.21, %3868, %3867, %3866, %3865, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.23 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.22), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3871 : Tensor = prim::GetAttr[name="weight"](%3856)
  %3872 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv2
  %3873 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv2
  %3874 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv2
  %3875 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv2
  %input.24 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.23, %3871, %3739, %3872, %3873, %3874, %3736, %3875, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3877 : Tensor = prim::GetAttr[name="running_var"](%3855)
  %3878 : Tensor = prim::GetAttr[name="running_mean"](%3855)
  %3879 : Tensor = prim::GetAttr[name="bias"](%3855)
  %3880 : Tensor = prim::GetAttr[name="weight"](%3855)
  %input.25 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.24, %3880, %3879, %3878, %3877, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.26 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.25), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3883 : Tensor = prim::GetAttr[name="weight"](%3854)
  %3884 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv3
  %3885 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv3
  %3886 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv3
  %3887 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv3
  %input.27 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.26, %3883, %3739, %3884, %3885, %3886, %3736, %3887, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3889 : Tensor = prim::GetAttr[name="running_var"](%3853)
  %3890 : Tensor = prim::GetAttr[name="running_mean"](%3853)
  %3891 : Tensor = prim::GetAttr[name="bias"](%3853)
  %3892 : Tensor = prim::GetAttr[name="weight"](%3853)
  %out.2 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.27, %3892, %3891, %3890, %3889, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.28 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::add_(%out.2, %input.20, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.29 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.28), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.1/__module.backbone.layer3.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3896 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%3793)
  %3897 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%3793)
  %3898 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_29.BatchNorm2d = prim::GetAttr[name="bn2"](%3793)
  %3899 : __torch__.torch.nn.modules.conv.___torch_mangle_28.Conv2d = prim::GetAttr[name="conv2"](%3793)
  %3900 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_27.BatchNorm2d = prim::GetAttr[name="bn1"](%3793)
  %3901 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv1"](%3793)
  %3902 : Tensor = prim::GetAttr[name="weight"](%3901)
  %3903 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv1
  %3904 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv1
  %3905 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv1
  %3906 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv1
  %input.30 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.29, %3902, %3739, %3903, %3904, %3905, %3736, %3906, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3908 : Tensor = prim::GetAttr[name="running_var"](%3900)
  %3909 : Tensor = prim::GetAttr[name="running_mean"](%3900)
  %3910 : Tensor = prim::GetAttr[name="bias"](%3900)
  %3911 : Tensor = prim::GetAttr[name="weight"](%3900)
  %input.31 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.30, %3911, %3910, %3909, %3908, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.32 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.31), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3914 : Tensor = prim::GetAttr[name="weight"](%3899)
  %3915 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv2
  %3916 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv2
  %3917 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv2
  %3918 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv2
  %input.33 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.32, %3914, %3739, %3915, %3916, %3917, %3736, %3918, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3920 : Tensor = prim::GetAttr[name="running_var"](%3898)
  %3921 : Tensor = prim::GetAttr[name="running_mean"](%3898)
  %3922 : Tensor = prim::GetAttr[name="bias"](%3898)
  %3923 : Tensor = prim::GetAttr[name="weight"](%3898)
  %input.34 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.33, %3923, %3922, %3921, %3920, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.35 : Float(2:1048576, 64:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.34), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3926 : Tensor = prim::GetAttr[name="weight"](%3897)
  %3927 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv3
  %3928 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv3
  %3929 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv3
  %3930 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv3
  %input.36 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.35, %3926, %3739, %3927, %3928, %3929, %3736, %3930, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3932 : Tensor = prim::GetAttr[name="running_var"](%3896)
  %3933 : Tensor = prim::GetAttr[name="running_mean"](%3896)
  %3934 : Tensor = prim::GetAttr[name="bias"](%3896)
  %3935 : Tensor = prim::GetAttr[name="weight"](%3896)
  %out.3 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.36, %3935, %3934, %3933, %3932, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.37 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::add_(%out.3, %input.29, %3738), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.38 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.37), scope: __module.backbone/__module.backbone.layer3/__module.backbone.layer3.2/__module.backbone.layer3.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3939 : __torch__.modeling.backbone.drn.___torch_mangle_69.Bottleneck = prim::GetAttr[name="3"](%3746)
  %3940 : __torch__.modeling.backbone.drn.___torch_mangle_61.Bottleneck = prim::GetAttr[name="2"](%3746)
  %3941 : __torch__.modeling.backbone.drn.___torch_mangle_53.Bottleneck = prim::GetAttr[name="1"](%3746)
  %3942 : __torch__.modeling.backbone.drn.___torch_mangle_45.Bottleneck = prim::GetAttr[name="0"](%3746)
  %3943 : __torch__.torch.nn.modules.container.___torch_mangle_44.Sequential = prim::GetAttr[name="downsample"](%3942)
  %3944 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_40.BatchNorm2d = prim::GetAttr[name="bn3"](%3942)
  %3945 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv3"](%3942)
  %3946 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_38.BatchNorm2d = prim::GetAttr[name="bn2"](%3942)
  %3947 : __torch__.torch.nn.modules.conv.___torch_mangle_37.Conv2d = prim::GetAttr[name="conv2"](%3942)
  %3948 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_36.BatchNorm2d = prim::GetAttr[name="bn1"](%3942)
  %3949 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%3942)
  %3950 : Tensor = prim::GetAttr[name="weight"](%3949)
  %3951 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv1
  %3952 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv1
  %3953 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv1
  %3954 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv1
  %input.39 : Float(2:2097152, 128:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.38, %3950, %3739, %3951, %3952, %3953, %3736, %3954, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3956 : Tensor = prim::GetAttr[name="running_var"](%3948)
  %3957 : Tensor = prim::GetAttr[name="running_mean"](%3948)
  %3958 : Tensor = prim::GetAttr[name="bias"](%3948)
  %3959 : Tensor = prim::GetAttr[name="weight"](%3948)
  %input.40 : Float(2:2097152, 128:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.39, %3959, %3958, %3957, %3956, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.41 : Float(2:2097152, 128:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu_(%input.40), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3962 : Tensor = prim::GetAttr[name="weight"](%3947)
  %3963 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv2
  %3964 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv2
  %3965 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv2
  %3966 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv2
  %input.42 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.41, %3962, %3739, %3963, %3964, %3965, %3736, %3966, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3968 : Tensor = prim::GetAttr[name="running_var"](%3946)
  %3969 : Tensor = prim::GetAttr[name="running_mean"](%3946)
  %3970 : Tensor = prim::GetAttr[name="bias"](%3946)
  %3971 : Tensor = prim::GetAttr[name="weight"](%3946)
  %input.43 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.42, %3971, %3970, %3969, %3968, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.44 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.43), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %3974 : Tensor = prim::GetAttr[name="weight"](%3945)
  %3975 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv3
  %3976 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv3
  %3977 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv3
  %3978 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv3
  %input.45 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.44, %3974, %3739, %3975, %3976, %3977, %3736, %3978, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3980 : Tensor = prim::GetAttr[name="running_var"](%3944)
  %3981 : Tensor = prim::GetAttr[name="running_mean"](%3944)
  %3982 : Tensor = prim::GetAttr[name="bias"](%3944)
  %3983 : Tensor = prim::GetAttr[name="weight"](%3944)
  %out.4 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.45, %3983, %3982, %3981, %3980, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %3985 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_43.BatchNorm2d = prim::GetAttr[name="1"](%3943)
  %3986 : __torch__.torch.nn.modules.conv.___torch_mangle_42.Conv2d = prim::GetAttr[name="0"](%3943)
  %3987 : Tensor = prim::GetAttr[name="weight"](%3986)
  %3988 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.downsample/__module.backbone.layer4.0.downsample.0
  %3989 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.downsample/__module.backbone.layer4.0.downsample.0
  %3990 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.downsample/__module.backbone.layer4.0.downsample.0
  %3991 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.downsample/__module.backbone.layer4.0.downsample.0
  %input.46 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.38, %3987, %3739, %3988, %3989, %3990, %3736, %3991, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.downsample/__module.backbone.layer4.0.downsample.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %3993 : Tensor = prim::GetAttr[name="running_var"](%3985)
  %3994 : Tensor = prim::GetAttr[name="running_mean"](%3985)
  %3995 : Tensor = prim::GetAttr[name="bias"](%3985)
  %3996 : Tensor = prim::GetAttr[name="weight"](%3985)
  %residual.2 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.46, %3996, %3995, %3994, %3993, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.downsample/__module.backbone.layer4.0.downsample.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.47 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.4, %residual.2, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.48 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.47), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.0/__module.backbone.layer4.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4000 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_51.BatchNorm2d = prim::GetAttr[name="bn3"](%3941)
  %4001 : __torch__.torch.nn.modules.conv.___torch_mangle_50.Conv2d = prim::GetAttr[name="conv3"](%3941)
  %4002 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_49.BatchNorm2d = prim::GetAttr[name="bn2"](%3941)
  %4003 : __torch__.torch.nn.modules.conv.___torch_mangle_48.Conv2d = prim::GetAttr[name="conv2"](%3941)
  %4004 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_47.BatchNorm2d = prim::GetAttr[name="bn1"](%3941)
  %4005 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name="conv1"](%3941)
  %4006 : Tensor = prim::GetAttr[name="weight"](%4005)
  %4007 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv1
  %4008 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv1
  %4009 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv1
  %4010 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv1
  %input.49 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.48, %4006, %3739, %4007, %4008, %4009, %3736, %4010, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4012 : Tensor = prim::GetAttr[name="running_var"](%4004)
  %4013 : Tensor = prim::GetAttr[name="running_mean"](%4004)
  %4014 : Tensor = prim::GetAttr[name="bias"](%4004)
  %4015 : Tensor = prim::GetAttr[name="weight"](%4004)
  %input.50 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.49, %4015, %4014, %4013, %4012, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.51 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.50), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4018 : Tensor = prim::GetAttr[name="weight"](%4003)
  %4019 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv2
  %4020 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv2
  %4021 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv2
  %4022 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv2
  %input.52 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.51, %4018, %3739, %4019, %4020, %4021, %3736, %4022, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4024 : Tensor = prim::GetAttr[name="running_var"](%4002)
  %4025 : Tensor = prim::GetAttr[name="running_mean"](%4002)
  %4026 : Tensor = prim::GetAttr[name="bias"](%4002)
  %4027 : Tensor = prim::GetAttr[name="weight"](%4002)
  %input.53 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.52, %4027, %4026, %4025, %4024, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.54 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.53), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4030 : Tensor = prim::GetAttr[name="weight"](%4001)
  %4031 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv3
  %4032 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv3
  %4033 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv3
  %4034 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv3
  %input.55 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.54, %4030, %3739, %4031, %4032, %4033, %3736, %4034, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4036 : Tensor = prim::GetAttr[name="running_var"](%4000)
  %4037 : Tensor = prim::GetAttr[name="running_mean"](%4000)
  %4038 : Tensor = prim::GetAttr[name="bias"](%4000)
  %4039 : Tensor = prim::GetAttr[name="weight"](%4000)
  %out.5 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.55, %4039, %4038, %4037, %4036, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.56 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.5, %input.48, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.57 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.56), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.1/__module.backbone.layer4.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4043 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_59.BatchNorm2d = prim::GetAttr[name="bn3"](%3940)
  %4044 : __torch__.torch.nn.modules.conv.___torch_mangle_58.Conv2d = prim::GetAttr[name="conv3"](%3940)
  %4045 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_57.BatchNorm2d = prim::GetAttr[name="bn2"](%3940)
  %4046 : __torch__.torch.nn.modules.conv.___torch_mangle_56.Conv2d = prim::GetAttr[name="conv2"](%3940)
  %4047 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_55.BatchNorm2d = prim::GetAttr[name="bn1"](%3940)
  %4048 : __torch__.torch.nn.modules.conv.___torch_mangle_54.Conv2d = prim::GetAttr[name="conv1"](%3940)
  %4049 : Tensor = prim::GetAttr[name="weight"](%4048)
  %4050 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv1
  %4051 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv1
  %4052 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv1
  %4053 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv1
  %input.58 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.57, %4049, %3739, %4050, %4051, %4052, %3736, %4053, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4055 : Tensor = prim::GetAttr[name="running_var"](%4047)
  %4056 : Tensor = prim::GetAttr[name="running_mean"](%4047)
  %4057 : Tensor = prim::GetAttr[name="bias"](%4047)
  %4058 : Tensor = prim::GetAttr[name="weight"](%4047)
  %input.59 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.58, %4058, %4057, %4056, %4055, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.60 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.59), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4061 : Tensor = prim::GetAttr[name="weight"](%4046)
  %4062 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv2
  %4063 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv2
  %4064 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv2
  %4065 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv2
  %input.61 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.60, %4061, %3739, %4062, %4063, %4064, %3736, %4065, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4067 : Tensor = prim::GetAttr[name="running_var"](%4045)
  %4068 : Tensor = prim::GetAttr[name="running_mean"](%4045)
  %4069 : Tensor = prim::GetAttr[name="bias"](%4045)
  %4070 : Tensor = prim::GetAttr[name="weight"](%4045)
  %input.62 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.61, %4070, %4069, %4068, %4067, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.63 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.62), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4073 : Tensor = prim::GetAttr[name="weight"](%4044)
  %4074 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv3
  %4075 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv3
  %4076 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv3
  %4077 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv3
  %input.64 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.63, %4073, %3739, %4074, %4075, %4076, %3736, %4077, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4079 : Tensor = prim::GetAttr[name="running_var"](%4043)
  %4080 : Tensor = prim::GetAttr[name="running_mean"](%4043)
  %4081 : Tensor = prim::GetAttr[name="bias"](%4043)
  %4082 : Tensor = prim::GetAttr[name="weight"](%4043)
  %out.6 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.64, %4082, %4081, %4080, %4079, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.65 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.6, %input.57, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.66 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.65), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.2/__module.backbone.layer4.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4086 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_67.BatchNorm2d = prim::GetAttr[name="bn3"](%3939)
  %4087 : __torch__.torch.nn.modules.conv.___torch_mangle_66.Conv2d = prim::GetAttr[name="conv3"](%3939)
  %4088 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_65.BatchNorm2d = prim::GetAttr[name="bn2"](%3939)
  %4089 : __torch__.torch.nn.modules.conv.___torch_mangle_64.Conv2d = prim::GetAttr[name="conv2"](%3939)
  %4090 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_63.BatchNorm2d = prim::GetAttr[name="bn1"](%3939)
  %4091 : __torch__.torch.nn.modules.conv.___torch_mangle_62.Conv2d = prim::GetAttr[name="conv1"](%3939)
  %4092 : Tensor = prim::GetAttr[name="weight"](%4091)
  %4093 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv1
  %4094 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv1
  %4095 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv1
  %4096 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv1
  %input.67 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.66, %4092, %3739, %4093, %4094, %4095, %3736, %4096, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4098 : Tensor = prim::GetAttr[name="running_var"](%4090)
  %4099 : Tensor = prim::GetAttr[name="running_mean"](%4090)
  %4100 : Tensor = prim::GetAttr[name="bias"](%4090)
  %4101 : Tensor = prim::GetAttr[name="weight"](%4090)
  %input.68 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.67, %4101, %4100, %4099, %4098, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.69 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.68), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4104 : Tensor = prim::GetAttr[name="weight"](%4089)
  %4105 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv2
  %4106 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv2
  %4107 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv2
  %4108 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv2
  %input.70 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.69, %4104, %3739, %4105, %4106, %4107, %3736, %4108, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4110 : Tensor = prim::GetAttr[name="running_var"](%4088)
  %4111 : Tensor = prim::GetAttr[name="running_mean"](%4088)
  %4112 : Tensor = prim::GetAttr[name="bias"](%4088)
  %4113 : Tensor = prim::GetAttr[name="weight"](%4088)
  %input.71 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.70, %4113, %4112, %4111, %4110, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.72 : Float(2:524288, 128:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.71), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4116 : Tensor = prim::GetAttr[name="weight"](%4087)
  %4117 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv3
  %4118 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv3
  %4119 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv3
  %4120 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv3
  %input.73 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.72, %4116, %3739, %4117, %4118, %4119, %3736, %4120, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4122 : Tensor = prim::GetAttr[name="running_var"](%4086)
  %4123 : Tensor = prim::GetAttr[name="running_mean"](%4086)
  %4124 : Tensor = prim::GetAttr[name="bias"](%4086)
  %4125 : Tensor = prim::GetAttr[name="weight"](%4086)
  %out.7 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.73, %4125, %4124, %4123, %4122, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.74 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.7, %input.66, %3738), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.75 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.74), scope: __module.backbone/__module.backbone.layer4/__module.backbone.layer4.3/__module.backbone.layer4.3.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4129 : __torch__.modeling.backbone.drn.___torch_mangle_121.Bottleneck = prim::GetAttr[name="5"](%3745)
  %4130 : __torch__.modeling.backbone.drn.___torch_mangle_113.Bottleneck = prim::GetAttr[name="4"](%3745)
  %4131 : __torch__.modeling.backbone.drn.___torch_mangle_105.Bottleneck = prim::GetAttr[name="3"](%3745)
  %4132 : __torch__.modeling.backbone.drn.___torch_mangle_97.Bottleneck = prim::GetAttr[name="2"](%3745)
  %4133 : __torch__.modeling.backbone.drn.___torch_mangle_89.Bottleneck = prim::GetAttr[name="1"](%3745)
  %4134 : __torch__.modeling.backbone.drn.___torch_mangle_81.Bottleneck = prim::GetAttr[name="0"](%3745)
  %4135 : __torch__.torch.nn.modules.container.___torch_mangle_80.Sequential = prim::GetAttr[name="downsample"](%4134)
  %4136 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_76.BatchNorm2d = prim::GetAttr[name="bn3"](%4134)
  %4137 : __torch__.torch.nn.modules.conv.___torch_mangle_75.Conv2d = prim::GetAttr[name="conv3"](%4134)
  %4138 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_74.BatchNorm2d = prim::GetAttr[name="bn2"](%4134)
  %4139 : __torch__.torch.nn.modules.conv.___torch_mangle_73.Conv2d = prim::GetAttr[name="conv2"](%4134)
  %4140 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_72.BatchNorm2d = prim::GetAttr[name="bn1"](%4134)
  %4141 : __torch__.torch.nn.modules.conv.___torch_mangle_71.Conv2d = prim::GetAttr[name="conv1"](%4134)
  %4142 : Tensor = prim::GetAttr[name="weight"](%4141)
  %4143 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv1
  %4144 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv1
  %4145 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv1
  %4146 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv1
  %input.76 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.75, %4142, %3739, %4143, %4144, %4145, %3736, %4146, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4148 : Tensor = prim::GetAttr[name="running_var"](%4140)
  %4149 : Tensor = prim::GetAttr[name="running_mean"](%4140)
  %4150 : Tensor = prim::GetAttr[name="bias"](%4140)
  %4151 : Tensor = prim::GetAttr[name="weight"](%4140)
  %input.77 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.76, %4151, %4150, %4149, %4148, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.78 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.77), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4154 : Tensor = prim::GetAttr[name="weight"](%4139)
  %4155 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv2
  %4156 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv2
  %4157 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv2
  %4158 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv2
  %input.79 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.78, %4154, %3739, %4155, %4156, %4157, %3736, %4158, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4160 : Tensor = prim::GetAttr[name="running_var"](%4138)
  %4161 : Tensor = prim::GetAttr[name="running_mean"](%4138)
  %4162 : Tensor = prim::GetAttr[name="bias"](%4138)
  %4163 : Tensor = prim::GetAttr[name="weight"](%4138)
  %input.80 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.79, %4163, %4162, %4161, %4160, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.81 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.80), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4166 : Tensor = prim::GetAttr[name="weight"](%4137)
  %4167 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv3
  %4168 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv3
  %4169 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv3
  %4170 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv3
  %input.82 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.81, %4166, %3739, %4167, %4168, %4169, %3736, %4170, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4172 : Tensor = prim::GetAttr[name="running_var"](%4136)
  %4173 : Tensor = prim::GetAttr[name="running_mean"](%4136)
  %4174 : Tensor = prim::GetAttr[name="bias"](%4136)
  %4175 : Tensor = prim::GetAttr[name="weight"](%4136)
  %out.8 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.82, %4175, %4174, %4173, %4172, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %4177 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_79.BatchNorm2d = prim::GetAttr[name="1"](%4135)
  %4178 : __torch__.torch.nn.modules.conv.___torch_mangle_78.Conv2d = prim::GetAttr[name="0"](%4135)
  %4179 : Tensor = prim::GetAttr[name="weight"](%4178)
  %4180 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.downsample/__module.backbone.layer5.0.downsample.0
  %4181 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.downsample/__module.backbone.layer5.0.downsample.0
  %4182 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.downsample/__module.backbone.layer5.0.downsample.0
  %4183 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.downsample/__module.backbone.layer5.0.downsample.0
  %input.83 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.75, %4179, %3739, %4180, %4181, %4182, %3736, %4183, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.downsample/__module.backbone.layer5.0.downsample.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4185 : Tensor = prim::GetAttr[name="running_var"](%4177)
  %4186 : Tensor = prim::GetAttr[name="running_mean"](%4177)
  %4187 : Tensor = prim::GetAttr[name="bias"](%4177)
  %4188 : Tensor = prim::GetAttr[name="weight"](%4177)
  %residual.3 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.83, %4188, %4187, %4186, %4185, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.downsample/__module.backbone.layer5.0.downsample.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.84 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.8, %residual.3, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.85 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.84), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.0/__module.backbone.layer5.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4192 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_87.BatchNorm2d = prim::GetAttr[name="bn3"](%4133)
  %4193 : __torch__.torch.nn.modules.conv.___torch_mangle_86.Conv2d = prim::GetAttr[name="conv3"](%4133)
  %4194 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_85.BatchNorm2d = prim::GetAttr[name="bn2"](%4133)
  %4195 : __torch__.torch.nn.modules.conv.___torch_mangle_84.Conv2d = prim::GetAttr[name="conv2"](%4133)
  %4196 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_83.BatchNorm2d = prim::GetAttr[name="bn1"](%4133)
  %4197 : __torch__.torch.nn.modules.conv.___torch_mangle_82.Conv2d = prim::GetAttr[name="conv1"](%4133)
  %4198 : Tensor = prim::GetAttr[name="weight"](%4197)
  %4199 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv1
  %4200 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv1
  %4201 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv1
  %4202 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv1
  %input.86 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.85, %4198, %3739, %4199, %4200, %4201, %3736, %4202, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4204 : Tensor = prim::GetAttr[name="running_var"](%4196)
  %4205 : Tensor = prim::GetAttr[name="running_mean"](%4196)
  %4206 : Tensor = prim::GetAttr[name="bias"](%4196)
  %4207 : Tensor = prim::GetAttr[name="weight"](%4196)
  %input.87 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.86, %4207, %4206, %4205, %4204, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.88 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.87), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4210 : Tensor = prim::GetAttr[name="weight"](%4195)
  %4211 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv2
  %4212 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv2
  %4213 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv2
  %4214 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv2
  %input.89 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.88, %4210, %3739, %4211, %4212, %4213, %3736, %4214, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4216 : Tensor = prim::GetAttr[name="running_var"](%4194)
  %4217 : Tensor = prim::GetAttr[name="running_mean"](%4194)
  %4218 : Tensor = prim::GetAttr[name="bias"](%4194)
  %4219 : Tensor = prim::GetAttr[name="weight"](%4194)
  %input.90 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.89, %4219, %4218, %4217, %4216, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.91 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.90), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4222 : Tensor = prim::GetAttr[name="weight"](%4193)
  %4223 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv3
  %4224 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv3
  %4225 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv3
  %4226 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv3
  %input.92 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.91, %4222, %3739, %4223, %4224, %4225, %3736, %4226, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4228 : Tensor = prim::GetAttr[name="running_var"](%4192)
  %4229 : Tensor = prim::GetAttr[name="running_mean"](%4192)
  %4230 : Tensor = prim::GetAttr[name="bias"](%4192)
  %4231 : Tensor = prim::GetAttr[name="weight"](%4192)
  %out.9 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.92, %4231, %4230, %4229, %4228, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.93 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.9, %input.85, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.94 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.93), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.1/__module.backbone.layer5.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4235 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_95.BatchNorm2d = prim::GetAttr[name="bn3"](%4132)
  %4236 : __torch__.torch.nn.modules.conv.___torch_mangle_94.Conv2d = prim::GetAttr[name="conv3"](%4132)
  %4237 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_93.BatchNorm2d = prim::GetAttr[name="bn2"](%4132)
  %4238 : __torch__.torch.nn.modules.conv.___torch_mangle_92.Conv2d = prim::GetAttr[name="conv2"](%4132)
  %4239 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="bn1"](%4132)
  %4240 : __torch__.torch.nn.modules.conv.___torch_mangle_90.Conv2d = prim::GetAttr[name="conv1"](%4132)
  %4241 : Tensor = prim::GetAttr[name="weight"](%4240)
  %4242 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv1
  %4243 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv1
  %4244 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv1
  %4245 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv1
  %input.95 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.94, %4241, %3739, %4242, %4243, %4244, %3736, %4245, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4247 : Tensor = prim::GetAttr[name="running_var"](%4239)
  %4248 : Tensor = prim::GetAttr[name="running_mean"](%4239)
  %4249 : Tensor = prim::GetAttr[name="bias"](%4239)
  %4250 : Tensor = prim::GetAttr[name="weight"](%4239)
  %input.96 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.95, %4250, %4249, %4248, %4247, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.97 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.96), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4253 : Tensor = prim::GetAttr[name="weight"](%4238)
  %4254 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv2
  %4255 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv2
  %4256 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv2
  %4257 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv2
  %input.98 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.97, %4253, %3739, %4254, %4255, %4256, %3736, %4257, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4259 : Tensor = prim::GetAttr[name="running_var"](%4237)
  %4260 : Tensor = prim::GetAttr[name="running_mean"](%4237)
  %4261 : Tensor = prim::GetAttr[name="bias"](%4237)
  %4262 : Tensor = prim::GetAttr[name="weight"](%4237)
  %input.99 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.98, %4262, %4261, %4260, %4259, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.100 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.99), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4265 : Tensor = prim::GetAttr[name="weight"](%4236)
  %4266 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv3
  %4267 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv3
  %4268 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv3
  %4269 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv3
  %input.101 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.100, %4265, %3739, %4266, %4267, %4268, %3736, %4269, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4271 : Tensor = prim::GetAttr[name="running_var"](%4235)
  %4272 : Tensor = prim::GetAttr[name="running_mean"](%4235)
  %4273 : Tensor = prim::GetAttr[name="bias"](%4235)
  %4274 : Tensor = prim::GetAttr[name="weight"](%4235)
  %out.10 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.101, %4274, %4273, %4272, %4271, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.102 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.10, %input.94, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.103 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.102), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.2/__module.backbone.layer5.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4278 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_103.BatchNorm2d = prim::GetAttr[name="bn3"](%4131)
  %4279 : __torch__.torch.nn.modules.conv.___torch_mangle_102.Conv2d = prim::GetAttr[name="conv3"](%4131)
  %4280 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_101.BatchNorm2d = prim::GetAttr[name="bn2"](%4131)
  %4281 : __torch__.torch.nn.modules.conv.___torch_mangle_100.Conv2d = prim::GetAttr[name="conv2"](%4131)
  %4282 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_99.BatchNorm2d = prim::GetAttr[name="bn1"](%4131)
  %4283 : __torch__.torch.nn.modules.conv.___torch_mangle_98.Conv2d = prim::GetAttr[name="conv1"](%4131)
  %4284 : Tensor = prim::GetAttr[name="weight"](%4283)
  %4285 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv1
  %4286 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv1
  %4287 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv1
  %4288 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv1
  %input.104 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.103, %4284, %3739, %4285, %4286, %4287, %3736, %4288, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4290 : Tensor = prim::GetAttr[name="running_var"](%4282)
  %4291 : Tensor = prim::GetAttr[name="running_mean"](%4282)
  %4292 : Tensor = prim::GetAttr[name="bias"](%4282)
  %4293 : Tensor = prim::GetAttr[name="weight"](%4282)
  %input.105 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.104, %4293, %4292, %4291, %4290, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.106 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.105), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4296 : Tensor = prim::GetAttr[name="weight"](%4281)
  %4297 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv2
  %4298 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv2
  %4299 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv2
  %4300 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv2
  %input.107 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.106, %4296, %3739, %4297, %4298, %4299, %3736, %4300, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4302 : Tensor = prim::GetAttr[name="running_var"](%4280)
  %4303 : Tensor = prim::GetAttr[name="running_mean"](%4280)
  %4304 : Tensor = prim::GetAttr[name="bias"](%4280)
  %4305 : Tensor = prim::GetAttr[name="weight"](%4280)
  %input.108 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.107, %4305, %4304, %4303, %4302, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.109 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.108), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4308 : Tensor = prim::GetAttr[name="weight"](%4279)
  %4309 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv3
  %4310 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv3
  %4311 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv3
  %4312 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv3
  %input.110 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.109, %4308, %3739, %4309, %4310, %4311, %3736, %4312, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4314 : Tensor = prim::GetAttr[name="running_var"](%4278)
  %4315 : Tensor = prim::GetAttr[name="running_mean"](%4278)
  %4316 : Tensor = prim::GetAttr[name="bias"](%4278)
  %4317 : Tensor = prim::GetAttr[name="weight"](%4278)
  %out.11 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.110, %4317, %4316, %4315, %4314, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.111 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.11, %input.103, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.112 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.111), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.3/__module.backbone.layer5.3.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4321 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_111.BatchNorm2d = prim::GetAttr[name="bn3"](%4130)
  %4322 : __torch__.torch.nn.modules.conv.___torch_mangle_110.Conv2d = prim::GetAttr[name="conv3"](%4130)
  %4323 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_109.BatchNorm2d = prim::GetAttr[name="bn2"](%4130)
  %4324 : __torch__.torch.nn.modules.conv.___torch_mangle_108.Conv2d = prim::GetAttr[name="conv2"](%4130)
  %4325 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_107.BatchNorm2d = prim::GetAttr[name="bn1"](%4130)
  %4326 : __torch__.torch.nn.modules.conv.___torch_mangle_106.Conv2d = prim::GetAttr[name="conv1"](%4130)
  %4327 : Tensor = prim::GetAttr[name="weight"](%4326)
  %4328 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv1
  %4329 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv1
  %4330 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv1
  %4331 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv1
  %input.113 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.112, %4327, %3739, %4328, %4329, %4330, %3736, %4331, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4333 : Tensor = prim::GetAttr[name="running_var"](%4325)
  %4334 : Tensor = prim::GetAttr[name="running_mean"](%4325)
  %4335 : Tensor = prim::GetAttr[name="bias"](%4325)
  %4336 : Tensor = prim::GetAttr[name="weight"](%4325)
  %input.114 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.113, %4336, %4335, %4334, %4333, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.115 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.114), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4339 : Tensor = prim::GetAttr[name="weight"](%4324)
  %4340 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv2
  %4341 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv2
  %4342 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv2
  %4343 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv2
  %input.116 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.115, %4339, %3739, %4340, %4341, %4342, %3736, %4343, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4345 : Tensor = prim::GetAttr[name="running_var"](%4323)
  %4346 : Tensor = prim::GetAttr[name="running_mean"](%4323)
  %4347 : Tensor = prim::GetAttr[name="bias"](%4323)
  %4348 : Tensor = prim::GetAttr[name="weight"](%4323)
  %input.117 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.116, %4348, %4347, %4346, %4345, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.118 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.117), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4351 : Tensor = prim::GetAttr[name="weight"](%4322)
  %4352 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv3
  %4353 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv3
  %4354 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv3
  %4355 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv3
  %input.119 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.118, %4351, %3739, %4352, %4353, %4354, %3736, %4355, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4357 : Tensor = prim::GetAttr[name="running_var"](%4321)
  %4358 : Tensor = prim::GetAttr[name="running_mean"](%4321)
  %4359 : Tensor = prim::GetAttr[name="bias"](%4321)
  %4360 : Tensor = prim::GetAttr[name="weight"](%4321)
  %out.12 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.119, %4360, %4359, %4358, %4357, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.120 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.12, %input.112, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.121 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.120), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.4/__module.backbone.layer5.4.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4364 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_119.BatchNorm2d = prim::GetAttr[name="bn3"](%4129)
  %4365 : __torch__.torch.nn.modules.conv.___torch_mangle_118.Conv2d = prim::GetAttr[name="conv3"](%4129)
  %4366 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_117.BatchNorm2d = prim::GetAttr[name="bn2"](%4129)
  %4367 : __torch__.torch.nn.modules.conv.___torch_mangle_116.Conv2d = prim::GetAttr[name="conv2"](%4129)
  %4368 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="bn1"](%4129)
  %4369 : __torch__.torch.nn.modules.conv.___torch_mangle_114.Conv2d = prim::GetAttr[name="conv1"](%4129)
  %4370 : Tensor = prim::GetAttr[name="weight"](%4369)
  %4371 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv1
  %4372 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv1
  %4373 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv1
  %4374 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv1
  %input.122 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.121, %4370, %3739, %4371, %4372, %4373, %3736, %4374, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4376 : Tensor = prim::GetAttr[name="running_var"](%4368)
  %4377 : Tensor = prim::GetAttr[name="running_mean"](%4368)
  %4378 : Tensor = prim::GetAttr[name="bias"](%4368)
  %4379 : Tensor = prim::GetAttr[name="weight"](%4368)
  %input.123 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.122, %4379, %4378, %4377, %4376, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.124 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.123), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4382 : Tensor = prim::GetAttr[name="weight"](%4367)
  %4383 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv2
  %4384 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv2
  %4385 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv2
  %4386 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv2
  %input.125 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.124, %4382, %3739, %4383, %4384, %4385, %3736, %4386, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4388 : Tensor = prim::GetAttr[name="running_var"](%4366)
  %4389 : Tensor = prim::GetAttr[name="running_mean"](%4366)
  %4390 : Tensor = prim::GetAttr[name="bias"](%4366)
  %4391 : Tensor = prim::GetAttr[name="weight"](%4366)
  %input.126 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.125, %4391, %4390, %4389, %4388, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.127 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.126), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4394 : Tensor = prim::GetAttr[name="weight"](%4365)
  %4395 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv3
  %4396 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv3
  %4397 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv3
  %4398 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv3
  %input.128 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.127, %4394, %3739, %4395, %4396, %4397, %3736, %4398, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4400 : Tensor = prim::GetAttr[name="running_var"](%4364)
  %4401 : Tensor = prim::GetAttr[name="running_mean"](%4364)
  %4402 : Tensor = prim::GetAttr[name="bias"](%4364)
  %4403 : Tensor = prim::GetAttr[name="weight"](%4364)
  %out.13 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.128, %4403, %4402, %4401, %4400, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.129 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.13, %input.121, %3738), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.130 : Float(2:4194304, 1024:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.129), scope: __module.backbone/__module.backbone.layer5/__module.backbone.layer5.5/__module.backbone.layer5.5.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4407 : __torch__.modeling.backbone.drn.___torch_mangle_149.Bottleneck = prim::GetAttr[name="2"](%3744)
  %4408 : __torch__.modeling.backbone.drn.___torch_mangle_141.Bottleneck = prim::GetAttr[name="1"](%3744)
  %4409 : __torch__.modeling.backbone.drn.___torch_mangle_133.Bottleneck = prim::GetAttr[name="0"](%3744)
  %4410 : __torch__.torch.nn.modules.container.___torch_mangle_132.Sequential = prim::GetAttr[name="downsample"](%4409)
  %4411 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_128.BatchNorm2d = prim::GetAttr[name="bn3"](%4409)
  %4412 : __torch__.torch.nn.modules.conv.___torch_mangle_127.Conv2d = prim::GetAttr[name="conv3"](%4409)
  %4413 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_126.BatchNorm2d = prim::GetAttr[name="bn2"](%4409)
  %4414 : __torch__.torch.nn.modules.conv.___torch_mangle_125.Conv2d = prim::GetAttr[name="conv2"](%4409)
  %4415 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="bn1"](%4409)
  %4416 : __torch__.torch.nn.modules.conv.___torch_mangle_123.Conv2d = prim::GetAttr[name="conv1"](%4409)
  %4417 : Tensor = prim::GetAttr[name="weight"](%4416)
  %4418 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv1
  %4419 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv1
  %4420 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv1
  %4421 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv1
  %input.131 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.130, %4417, %3739, %4418, %4419, %4420, %3736, %4421, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4423 : Tensor = prim::GetAttr[name="running_var"](%4415)
  %4424 : Tensor = prim::GetAttr[name="running_mean"](%4415)
  %4425 : Tensor = prim::GetAttr[name="bias"](%4415)
  %4426 : Tensor = prim::GetAttr[name="weight"](%4415)
  %input.132 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.131, %4426, %4425, %4424, %4423, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.133 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.132), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4429 : Tensor = prim::GetAttr[name="weight"](%4414)
  %4430 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv2
  %4431 : int[] = prim::ListConstruct(%3732, %3732), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv2
  %4432 : int[] = prim::ListConstruct(%3732, %3732), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv2
  %4433 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv2
  %input.134 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.133, %4429, %3739, %4430, %4431, %4432, %3736, %4433, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4435 : Tensor = prim::GetAttr[name="running_var"](%4413)
  %4436 : Tensor = prim::GetAttr[name="running_mean"](%4413)
  %4437 : Tensor = prim::GetAttr[name="bias"](%4413)
  %4438 : Tensor = prim::GetAttr[name="weight"](%4413)
  %input.135 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.134, %4438, %4437, %4436, %4435, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.136 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.135), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4441 : Tensor = prim::GetAttr[name="weight"](%4412)
  %4442 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv3
  %4443 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv3
  %4444 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv3
  %4445 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv3
  %input.137 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.136, %4441, %3739, %4442, %4443, %4444, %3736, %4445, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4447 : Tensor = prim::GetAttr[name="running_var"](%4411)
  %4448 : Tensor = prim::GetAttr[name="running_mean"](%4411)
  %4449 : Tensor = prim::GetAttr[name="bias"](%4411)
  %4450 : Tensor = prim::GetAttr[name="weight"](%4411)
  %out.14 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.137, %4450, %4449, %4448, %4447, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %4452 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_131.BatchNorm2d = prim::GetAttr[name="1"](%4410)
  %4453 : __torch__.torch.nn.modules.conv.___torch_mangle_130.Conv2d = prim::GetAttr[name="0"](%4410)
  %4454 : Tensor = prim::GetAttr[name="weight"](%4453)
  %4455 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.downsample/__module.backbone.layer6.0.downsample.0
  %4456 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.downsample/__module.backbone.layer6.0.downsample.0
  %4457 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.downsample/__module.backbone.layer6.0.downsample.0
  %4458 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.downsample/__module.backbone.layer6.0.downsample.0
  %input.138 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.130, %4454, %3739, %4455, %4456, %4457, %3736, %4458, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.downsample/__module.backbone.layer6.0.downsample.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4460 : Tensor = prim::GetAttr[name="running_var"](%4452)
  %4461 : Tensor = prim::GetAttr[name="running_mean"](%4452)
  %4462 : Tensor = prim::GetAttr[name="bias"](%4452)
  %4463 : Tensor = prim::GetAttr[name="weight"](%4452)
  %residual : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.138, %4463, %4462, %4461, %4460, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.downsample/__module.backbone.layer6.0.downsample.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.139 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.14, %residual, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.140 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.139), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.0/__module.backbone.layer6.0.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4467 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_139.BatchNorm2d = prim::GetAttr[name="bn3"](%4408)
  %4468 : __torch__.torch.nn.modules.conv.___torch_mangle_138.Conv2d = prim::GetAttr[name="conv3"](%4408)
  %4469 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_137.BatchNorm2d = prim::GetAttr[name="bn2"](%4408)
  %4470 : __torch__.torch.nn.modules.conv.___torch_mangle_136.Conv2d = prim::GetAttr[name="conv2"](%4408)
  %4471 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_135.BatchNorm2d = prim::GetAttr[name="bn1"](%4408)
  %4472 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv1"](%4408)
  %4473 : Tensor = prim::GetAttr[name="weight"](%4472)
  %4474 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv1
  %4475 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv1
  %4476 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv1
  %4477 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv1
  %input.141 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.140, %4473, %3739, %4474, %4475, %4476, %3736, %4477, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4479 : Tensor = prim::GetAttr[name="running_var"](%4471)
  %4480 : Tensor = prim::GetAttr[name="running_mean"](%4471)
  %4481 : Tensor = prim::GetAttr[name="bias"](%4471)
  %4482 : Tensor = prim::GetAttr[name="weight"](%4471)
  %input.142 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.141, %4482, %4481, %4480, %4479, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.143 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.142), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4485 : Tensor = prim::GetAttr[name="weight"](%4470)
  %4486 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv2
  %4487 : int[] = prim::ListConstruct(%3732, %3732), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv2
  %4488 : int[] = prim::ListConstruct(%3732, %3732), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv2
  %4489 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv2
  %input.144 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.143, %4485, %3739, %4486, %4487, %4488, %3736, %4489, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4491 : Tensor = prim::GetAttr[name="running_var"](%4469)
  %4492 : Tensor = prim::GetAttr[name="running_mean"](%4469)
  %4493 : Tensor = prim::GetAttr[name="bias"](%4469)
  %4494 : Tensor = prim::GetAttr[name="weight"](%4469)
  %input.145 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.144, %4494, %4493, %4492, %4491, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.146 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.145), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4497 : Tensor = prim::GetAttr[name="weight"](%4468)
  %4498 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv3
  %4499 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv3
  %4500 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv3
  %4501 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv3
  %input.147 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.146, %4497, %3739, %4498, %4499, %4500, %3736, %4501, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4503 : Tensor = prim::GetAttr[name="running_var"](%4467)
  %4504 : Tensor = prim::GetAttr[name="running_mean"](%4467)
  %4505 : Tensor = prim::GetAttr[name="bias"](%4467)
  %4506 : Tensor = prim::GetAttr[name="weight"](%4467)
  %out.15 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.147, %4506, %4505, %4504, %4503, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.148 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out.15, %input.140, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.149 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.148), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.1/__module.backbone.layer6.1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4510 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_147.BatchNorm2d = prim::GetAttr[name="bn3"](%4407)
  %4511 : __torch__.torch.nn.modules.conv.___torch_mangle_146.Conv2d = prim::GetAttr[name="conv3"](%4407)
  %4512 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="bn2"](%4407)
  %4513 : __torch__.torch.nn.modules.conv.___torch_mangle_144.Conv2d = prim::GetAttr[name="conv2"](%4407)
  %4514 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_143.BatchNorm2d = prim::GetAttr[name="bn1"](%4407)
  %4515 : __torch__.torch.nn.modules.conv.___torch_mangle_142.Conv2d = prim::GetAttr[name="conv1"](%4407)
  %4516 : Tensor = prim::GetAttr[name="weight"](%4515)
  %4517 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv1
  %4518 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv1
  %4519 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv1
  %4520 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv1
  %input.150 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.149, %4516, %3739, %4517, %4518, %4519, %3736, %4520, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4522 : Tensor = prim::GetAttr[name="running_var"](%4514)
  %4523 : Tensor = prim::GetAttr[name="running_mean"](%4514)
  %4524 : Tensor = prim::GetAttr[name="bias"](%4514)
  %4525 : Tensor = prim::GetAttr[name="weight"](%4514)
  %input.151 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.150, %4525, %4524, %4523, %4522, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.152 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.151), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4528 : Tensor = prim::GetAttr[name="weight"](%4513)
  %4529 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv2
  %4530 : int[] = prim::ListConstruct(%3732, %3732), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv2
  %4531 : int[] = prim::ListConstruct(%3732, %3732), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv2
  %4532 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv2
  %input.153 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.152, %4528, %3739, %4529, %4530, %4531, %3736, %4532, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4534 : Tensor = prim::GetAttr[name="running_var"](%4512)
  %4535 : Tensor = prim::GetAttr[name="running_mean"](%4512)
  %4536 : Tensor = prim::GetAttr[name="bias"](%4512)
  %4537 : Tensor = prim::GetAttr[name="weight"](%4512)
  %input.154 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.153, %4537, %4536, %4535, %4534, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.bn2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.155 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.154), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4540 : Tensor = prim::GetAttr[name="weight"](%4511)
  %4541 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv3
  %4542 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv3
  %4543 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv3
  %4544 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv3
  %input.156 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.155, %4540, %3739, %4541, %4542, %4543, %3736, %4544, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.conv3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4546 : Tensor = prim::GetAttr[name="running_var"](%4510)
  %4547 : Tensor = prim::GetAttr[name="running_mean"](%4510)
  %4548 : Tensor = prim::GetAttr[name="bias"](%4510)
  %4549 : Tensor = prim::GetAttr[name="weight"](%4510)
  %out : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.156, %4549, %4548, %4547, %4546, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.bn3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.157 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::add_(%out, %input.149, %3738), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2 # D:\00code\small_obstacle_discovery-master\modeling\backbone\drn.py:96:0
  %input.158 : Float(2:8388608, 2048:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.157), scope: __module.backbone/__module.backbone.layer6/__module.backbone.layer6.2/__module.backbone.layer6.2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4553 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_152.BatchNorm2d = prim::GetAttr[name="1"](%3743)
  %4554 : __torch__.torch.nn.modules.conv.___torch_mangle_151.Conv2d = prim::GetAttr[name="0"](%3743)
  %4555 : Tensor = prim::GetAttr[name="weight"](%4554)
  %4556 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer7/__module.backbone.layer7.0
  %4557 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer7/__module.backbone.layer7.0
  %4558 : int[] = prim::ListConstruct(%3733, %3733), scope: __module.backbone/__module.backbone.layer7/__module.backbone.layer7.0
  %4559 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer7/__module.backbone.layer7.0
  %input.159 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.158, %4555, %3739, %4556, %4557, %4558, %3736, %4559, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer7/__module.backbone.layer7.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4561 : Tensor = prim::GetAttr[name="running_var"](%4553)
  %4562 : Tensor = prim::GetAttr[name="running_mean"](%4553)
  %4563 : Tensor = prim::GetAttr[name="bias"](%4553)
  %4564 : Tensor = prim::GetAttr[name="weight"](%4553)
  %input.160 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.159, %4564, %4563, %4562, %4561, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer7/__module.backbone.layer7.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.161 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.160), scope: __module.backbone/__module.backbone.layer7/__module.backbone.layer7.2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4567 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_156.BatchNorm2d = prim::GetAttr[name="1"](%3742)
  %4568 : __torch__.torch.nn.modules.conv.___torch_mangle_155.Conv2d = prim::GetAttr[name="0"](%3742)
  %4569 : Tensor = prim::GetAttr[name="weight"](%4568)
  %4570 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer8/__module.backbone.layer8.0
  %4571 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer8/__module.backbone.layer8.0
  %4572 : int[] = prim::ListConstruct(%3738, %3738), scope: __module.backbone/__module.backbone.layer8/__module.backbone.layer8.0
  %4573 : int[] = prim::ListConstruct(%3735, %3735), scope: __module.backbone/__module.backbone.layer8/__module.backbone.layer8.0
  %input.162 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.161, %4569, %3739, %4570, %4571, %4572, %3736, %4573, %3738, %3736, %3736, %3734, %3734), scope: __module.backbone/__module.backbone.layer8/__module.backbone.layer8.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4575 : Tensor = prim::GetAttr[name="running_var"](%4567)
  %4576 : Tensor = prim::GetAttr[name="running_mean"](%4567)
  %4577 : Tensor = prim::GetAttr[name="bias"](%4567)
  %4578 : Tensor = prim::GetAttr[name="weight"](%4567)
  %input.163 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.162, %4578, %4577, %4576, %4575, %3736, %3741, %3740, %3734), scope: __module.backbone/__module.backbone.layer8/__module.backbone.layer8.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.164 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu_(%input.163), scope: __module.backbone/__module.backbone.layer8/__module.backbone.layer8.2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1134:0
  %4581 : (Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu), Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.164, %input.38)
  %3723 : Float(2:2097152, 512:4096, 64:64, 64:1, requires_grad=1, device=cpu), %3724 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%4581)
  %4582 : float = prim::Constant[value=0.5](), scope: __module.aspp/__module.aspp.dropout # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:983:0
  %4583 : int = prim::Constant[value=3](), scope: __module.aspp # D:\00code\small_obstacle_discovery-master\modeling\aspp.py:71:0
  %4584 : int = prim::Constant[value=2](), scope: __module.aspp # D:\00code\small_obstacle_discovery-master\modeling\aspp.py:71:0
  %4585 : int = prim::Constant[value=36](), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4586 : int = prim::Constant[value=24](), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4587 : int = prim::Constant[value=12](), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4588 : bool = prim::Constant[value=1](), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4589 : bool = prim::Constant[value=0](), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4590 : int = prim::Constant[value=0](), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4591 : int = prim::Constant[value=1](), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4592 : None = prim::Constant(), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv
  %4593 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.bn # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %4594 : float = prim::Constant[value=0.10000000000000001](), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.bn # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %4595 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_179.BatchNorm2d = prim::GetAttr[name="bn1"](%3446)
  %4596 : __torch__.torch.nn.modules.conv.___torch_mangle_178.Conv2d = prim::GetAttr[name="conv1"](%3446)
  %4597 : __torch__.torch.nn.modules.container.___torch_mangle_177.Sequential = prim::GetAttr[name="global_avg_pool"](%3446)
  %4598 : __torch__.modeling.aspp.___torch_mangle_173._ASPPModule = prim::GetAttr[name="aspp4"](%3446)
  %4599 : __torch__.modeling.aspp.___torch_mangle_169._ASPPModule = prim::GetAttr[name="aspp3"](%3446)
  %4600 : __torch__.modeling.aspp.___torch_mangle_165._ASPPModule = prim::GetAttr[name="aspp2"](%3446)
  %4601 : __torch__.modeling.aspp._ASPPModule = prim::GetAttr[name="aspp1"](%3446)
  %4602 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_160.BatchNorm2d = prim::GetAttr[name="bn"](%4601)
  %4603 : __torch__.torch.nn.modules.conv.___torch_mangle_159.Conv2d = prim::GetAttr[name="atrous_conv"](%4601)
  %4604 : Tensor = prim::GetAttr[name="weight"](%4603)
  %4605 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv
  %4606 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv
  %4607 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv
  %4608 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv
  %input.165 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%3723, %4604, %4592, %4605, %4606, %4607, %4589, %4608, %4591, %4589, %4589, %4588, %4588), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4610 : Tensor = prim::GetAttr[name="running_var"](%4602)
  %4611 : Tensor = prim::GetAttr[name="running_mean"](%4602)
  %4612 : Tensor = prim::GetAttr[name="bias"](%4602)
  %4613 : Tensor = prim::GetAttr[name="weight"](%4602)
  %input.166 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.165, %4613, %4612, %4611, %4610, %4589, %4594, %4593, %4588), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.bn # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %x1 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu(%input.166), scope: __module.aspp/__module.aspp.aspp1/__module.aspp.aspp1.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %4616 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_163.BatchNorm2d = prim::GetAttr[name="bn"](%4600)
  %4617 : __torch__.torch.nn.modules.conv.___torch_mangle_162.Conv2d = prim::GetAttr[name="atrous_conv"](%4600)
  %4618 : Tensor = prim::GetAttr[name="weight"](%4617)
  %4619 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.atrous_conv
  %4620 : int[] = prim::ListConstruct(%4587, %4587), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.atrous_conv
  %4621 : int[] = prim::ListConstruct(%4587, %4587), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.atrous_conv
  %4622 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.atrous_conv
  %input.167 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%3723, %4618, %4592, %4619, %4620, %4621, %4589, %4622, %4591, %4589, %4589, %4588, %4588), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4624 : Tensor = prim::GetAttr[name="running_var"](%4616)
  %4625 : Tensor = prim::GetAttr[name="running_mean"](%4616)
  %4626 : Tensor = prim::GetAttr[name="bias"](%4616)
  %4627 : Tensor = prim::GetAttr[name="weight"](%4616)
  %input.168 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.167, %4627, %4626, %4625, %4624, %4589, %4594, %4593, %4588), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.bn # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %x2 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu(%input.168), scope: __module.aspp/__module.aspp.aspp2/__module.aspp.aspp2.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %4630 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_167.BatchNorm2d = prim::GetAttr[name="bn"](%4599)
  %4631 : __torch__.torch.nn.modules.conv.___torch_mangle_166.Conv2d = prim::GetAttr[name="atrous_conv"](%4599)
  %4632 : Tensor = prim::GetAttr[name="weight"](%4631)
  %4633 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.atrous_conv
  %4634 : int[] = prim::ListConstruct(%4586, %4586), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.atrous_conv
  %4635 : int[] = prim::ListConstruct(%4586, %4586), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.atrous_conv
  %4636 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.atrous_conv
  %input.169 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%3723, %4632, %4592, %4633, %4634, %4635, %4589, %4636, %4591, %4589, %4589, %4588, %4588), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4638 : Tensor = prim::GetAttr[name="running_var"](%4630)
  %4639 : Tensor = prim::GetAttr[name="running_mean"](%4630)
  %4640 : Tensor = prim::GetAttr[name="bias"](%4630)
  %4641 : Tensor = prim::GetAttr[name="weight"](%4630)
  %input.170 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.169, %4641, %4640, %4639, %4638, %4589, %4594, %4593, %4588), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.bn # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %x3 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu(%input.170), scope: __module.aspp/__module.aspp.aspp3/__module.aspp.aspp3.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %4644 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_171.BatchNorm2d = prim::GetAttr[name="bn"](%4598)
  %4645 : __torch__.torch.nn.modules.conv.___torch_mangle_170.Conv2d = prim::GetAttr[name="atrous_conv"](%4598)
  %4646 : Tensor = prim::GetAttr[name="weight"](%4645)
  %4647 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.atrous_conv
  %4648 : int[] = prim::ListConstruct(%4585, %4585), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.atrous_conv
  %4649 : int[] = prim::ListConstruct(%4585, %4585), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.atrous_conv
  %4650 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.atrous_conv
  %input.171 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%3723, %4646, %4592, %4647, %4648, %4649, %4589, %4650, %4591, %4589, %4589, %4588, %4588), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.atrous_conv # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4652 : Tensor = prim::GetAttr[name="running_var"](%4644)
  %4653 : Tensor = prim::GetAttr[name="running_mean"](%4644)
  %4654 : Tensor = prim::GetAttr[name="bias"](%4644)
  %4655 : Tensor = prim::GetAttr[name="weight"](%4644)
  %input.172 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.171, %4655, %4654, %4653, %4652, %4589, %4594, %4593, %4588), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.bn # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %x4 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu(%input.172), scope: __module.aspp/__module.aspp.aspp4/__module.aspp.aspp4.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %4658 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_175.BatchNorm2d = prim::GetAttr[name="2"](%4597)
  %4659 : __torch__.torch.nn.modules.conv.___torch_mangle_174.Conv2d = prim::GetAttr[name="1"](%4597)
  %4660 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.0
  %input.173 : Float(2:512, 512:1, 1:1, 1:1, requires_grad=1, device=cpu) = aten::adaptive_avg_pool2d(%3723, %4660), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:936:0
  %4662 : Tensor = prim::GetAttr[name="weight"](%4659)
  %4663 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.1
  %4664 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.1
  %4665 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.1
  %4666 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.1
  %input.174 : Float(2:256, 256:1, 1:1, 1:1, requires_grad=1, device=cpu) = aten::_convolution(%input.173, %4662, %4592, %4663, %4664, %4665, %4589, %4666, %4591, %4589, %4589, %4588, %4588), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4668 : Tensor = prim::GetAttr[name="running_var"](%4658)
  %4669 : Tensor = prim::GetAttr[name="running_mean"](%4658)
  %4670 : Tensor = prim::GetAttr[name="bias"](%4658)
  %4671 : Tensor = prim::GetAttr[name="weight"](%4658)
  %input.175 : Float(2:256, 256:1, 1:1, 1:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.174, %4671, %4670, %4669, %4668, %4589, %4594, %4593, %4588), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.176 : Float(2:256, 256:1, 1:1, 1:1, requires_grad=1, device=cpu) = aten::relu(%input.175), scope: __module.aspp/__module.aspp.global_avg_pool/__module.aspp.global_avg_pool.3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %4674 : int = aten::size(%x4, %4584), scope: __module.aspp # D:\00code\small_obstacle_discovery-master\modeling\aspp.py:71:0
  %4675 : int = aten::size(%x4, %4583), scope: __module.aspp # D:\00code\small_obstacle_discovery-master\modeling\aspp.py:71:0
  %4676 : int[] = prim::ListConstruct(%4674, %4675), scope: __module.aspp
  %x5 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::upsample_bilinear2d(%input.176, %4676, %4588, %4592), scope: __module.aspp # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:3151:0
  %4678 : Tensor[] = prim::ListConstruct(%x1, %x2, %x3, %x4, %x5), scope: __module.aspp
  %input.177 : Float(2:5242880, 1280:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::cat(%4678, %4591), scope: __module.aspp # D:\00code\small_obstacle_discovery-master\modeling\aspp.py:72:0
  %4680 : Tensor = prim::GetAttr[name="weight"](%4596)
  %4681 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.conv1
  %4682 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.conv1
  %4683 : int[] = prim::ListConstruct(%4591, %4591), scope: __module.aspp/__module.aspp.conv1
  %4684 : int[] = prim::ListConstruct(%4590, %4590), scope: __module.aspp/__module.aspp.conv1
  %input.178 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::_convolution(%input.177, %4680, %4592, %4681, %4682, %4683, %4589, %4684, %4591, %4589, %4589, %4588, %4588), scope: __module.aspp/__module.aspp.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4686 : Tensor = prim::GetAttr[name="running_var"](%4595)
  %4687 : Tensor = prim::GetAttr[name="running_mean"](%4595)
  %4688 : Tensor = prim::GetAttr[name="bias"](%4595)
  %4689 : Tensor = prim::GetAttr[name="weight"](%4595)
  %input.179 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.178, %4689, %4688, %4687, %4686, %4589, %4594, %4593, %4588), scope: __module.aspp/__module.aspp.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.180 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::relu(%input.179), scope: __module.aspp/__module.aspp.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %input.183 : Float(2:1048576, 256:4096, 64:64, 64:1, requires_grad=1, device=cpu) = aten::dropout(%input.180, %4582, %4589), scope: __module.aspp/__module.aspp.dropout # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:983:0
  %4693 : float = prim::Constant[value=0.5](), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:983:0
  %4694 : int = prim::Constant[value=3](), scope: __module.decoder # D:\00code\small_obstacle_discovery-master\modeling\decoder.py:47:0
  %4695 : int = prim::Constant[value=2](), scope: __module.decoder # D:\00code\small_obstacle_discovery-master\modeling\decoder.py:47:0
  %4696 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %4697 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.decoder/__module.decoder.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %4698 : None = prim::Constant(), scope: __module.decoder/__module.decoder.conv1
  %4699 : int = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4700 : int = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4701 : bool = prim::Constant[value=0](), scope: __module.decoder/__module.decoder.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4702 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4703 : __torch__.torch.nn.modules.conv.___torch_mangle_194.Conv2d = prim::GetAttr[name="diverge_conv_conf"](%3480)
  %4704 : __torch__.torch.nn.modules.conv.___torch_mangle_193.Conv2d = prim::GetAttr[name="diverge_conv_pred"](%3480)
  %4705 : __torch__.torch.nn.modules.container.___torch_mangle_192.Sequential = prim::GetAttr[name="last_conv"](%3480)
  %4706 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_182.BatchNorm2d = prim::GetAttr[name="bn1"](%3480)
  %4707 : __torch__.torch.nn.modules.conv.___torch_mangle_181.Conv2d = prim::GetAttr[name="conv1"](%3480)
  %4708 : Tensor = prim::GetAttr[name="weight"](%4707)
  %4709 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.conv1
  %4710 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.conv1
  %4711 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.conv1
  %4712 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.conv1
  %input.181 : Float(2:786432, 48:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%3724, %4708, %4698, %4709, %4710, %4711, %4701, %4712, %4699, %4701, %4701, %4702, %4702), scope: __module.decoder/__module.decoder.conv1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4714 : Tensor = prim::GetAttr[name="running_var"](%4706)
  %4715 : Tensor = prim::GetAttr[name="running_mean"](%4706)
  %4716 : Tensor = prim::GetAttr[name="bias"](%4706)
  %4717 : Tensor = prim::GetAttr[name="weight"](%4706)
  %input.182 : Float(2:786432, 48:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.181, %4717, %4716, %4715, %4714, %4701, %4696, %4697, %4702), scope: __module.decoder/__module.decoder.bn1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %low_level_feat : Float(2:786432, 48:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu(%input.182), scope: __module.decoder/__module.decoder.relu # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %4720 : int = aten::size(%low_level_feat, %4695), scope: __module.decoder # D:\00code\small_obstacle_discovery-master\modeling\decoder.py:47:0
  %4721 : int = aten::size(%low_level_feat, %4694), scope: __module.decoder # D:\00code\small_obstacle_discovery-master\modeling\decoder.py:47:0
  %4722 : int[] = prim::ListConstruct(%4720, %4721), scope: __module.decoder
  %x : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::upsample_bilinear2d(%input.183, %4722, %4702, %4698), scope: __module.decoder # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:3151:0
  %4724 : Tensor[] = prim::ListConstruct(%x, %low_level_feat), scope: __module.decoder
  %input.184 : Float(2:4980736, 304:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::cat(%4724, %4699), scope: __module.decoder # D:\00code\small_obstacle_discovery-master\modeling\decoder.py:48:0
  %4726 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_189.BatchNorm2d = prim::GetAttr[name="5"](%4705)
  %4727 : __torch__.torch.nn.modules.conv.___torch_mangle_188.Conv2d = prim::GetAttr[name="4"](%4705)
  %4728 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_185.BatchNorm2d = prim::GetAttr[name="1"](%4705)
  %4729 : __torch__.torch.nn.modules.conv.___torch_mangle_184.Conv2d = prim::GetAttr[name="0"](%4705)
  %4730 : Tensor = prim::GetAttr[name="weight"](%4729)
  %4731 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.0
  %4732 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.0
  %4733 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.0
  %4734 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.0
  %input.185 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.184, %4730, %4698, %4731, %4732, %4733, %4701, %4734, %4699, %4701, %4701, %4702, %4702), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.0 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4736 : Tensor = prim::GetAttr[name="running_var"](%4728)
  %4737 : Tensor = prim::GetAttr[name="running_mean"](%4728)
  %4738 : Tensor = prim::GetAttr[name="bias"](%4728)
  %4739 : Tensor = prim::GetAttr[name="weight"](%4728)
  %input.186 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.185, %4739, %4738, %4737, %4736, %4701, %4696, %4697, %4702), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.1 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.187 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu(%input.186), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.2 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %input.188 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::dropout(%input.187, %4693, %4701), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.3 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:983:0
  %4743 : Tensor = prim::GetAttr[name="weight"](%4727)
  %4744 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.4
  %4745 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.4
  %4746 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.4
  %4747 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.4
  %input.189 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.188, %4743, %4698, %4744, %4745, %4746, %4701, %4747, %4699, %4701, %4701, %4702, %4702), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.4 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4749 : Tensor = prim::GetAttr[name="running_var"](%4726)
  %4750 : Tensor = prim::GetAttr[name="running_mean"](%4726)
  %4751 : Tensor = prim::GetAttr[name="bias"](%4726)
  %4752 : Tensor = prim::GetAttr[name="weight"](%4726)
  %input.190 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::batch_norm(%input.189, %4752, %4751, %4750, %4749, %4701, %4696, %4697, %4702), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.5 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:2056:0
  %input.191 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::relu(%input.190), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.6 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1136:0
  %input.192 : Float(2:4194304, 256:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::dropout(%input.191, %4696, %4701), scope: __module.decoder/__module.decoder.last_conv/__module.decoder.last_conv.7 # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:983:0
  %4756 : Tensor = prim::GetAttr[name="bias"](%4704)
  %4757 : Tensor = prim::GetAttr[name="weight"](%4704)
  %4758 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.diverge_conv_pred
  %4759 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.diverge_conv_pred
  %4760 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.diverge_conv_pred
  %4761 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.diverge_conv_pred
  %input.193 : Float(2:49152, 3:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.192, %4757, %4756, %4758, %4759, %4760, %4701, %4761, %4699, %4701, %4701, %4702, %4702), scope: __module.decoder/__module.decoder.diverge_conv_pred # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4763 : Tensor = prim::GetAttr[name="bias"](%4703)
  %4764 : Tensor = prim::GetAttr[name="weight"](%4703)
  %4765 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.diverge_conv_conf
  %4766 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.diverge_conv_conf
  %4767 : int[] = prim::ListConstruct(%4699, %4699), scope: __module.decoder/__module.decoder.diverge_conv_conf
  %4768 : int[] = prim::ListConstruct(%4700, %4700), scope: __module.decoder/__module.decoder.diverge_conv_conf
  %input.194 : Float(2:16384, 1:16384, 128:128, 128:1, requires_grad=1, device=cpu) = aten::_convolution(%input.192, %4764, %4763, %4765, %4766, %4767, %4701, %4768, %4699, %4701, %4701, %4702, %4702), scope: __module.decoder/__module.decoder.diverge_conv_conf # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\modules\conv.py:419:0
  %4770 : (Float(2:49152, 3:16384, 128:128, 128:1, requires_grad=1, device=cpu), Float(2:16384, 1:16384, 128:128, 128:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%input.193, %input.194)
  %3727 : Float(2:49152, 3:16384, 128:128, 128:1, requires_grad=1, device=cpu), %3728 : Float(2:16384, 1:16384, 128:128, 128:1, requires_grad=1, device=cpu) = prim::TupleUnpack(%4770)
  %2670 : int = prim::Constant[value=2]() # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:43:0
  %2671 : int = aten::size(%input.1, %2670) # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:43:0
  %2672 : Long(device=cpu) = prim::NumToTensor(%2671)
  %2676 : int = aten::Int(%2672)
  %2673 : int = prim::Constant[value=3]() # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:43:0
  %2674 : int = aten::size(%input.1, %2673) # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:43:0
  %2675 : Long(device=cpu) = prim::NumToTensor(%2674)
  %2677 : int = aten::Int(%2675)
  %2678 : int[] = prim::ListConstruct(%2676, %2677)
  %2679 : bool = prim::Constant[value=1]() # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:3151:0
  %2680 : None = prim::Constant()
  %pre_conf : Float(2:786432, 3:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::upsample_bilinear2d(%3727, %2678, %2679, %2680) # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:3151:0
  %2688 : int = prim::Constant[value=2]() # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:46:0
  %2689 : int = aten::size(%input.1, %2688) # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:46:0
  %2690 : Long(device=cpu) = prim::NumToTensor(%2689)
  %2694 : int = aten::Int(%2690)
  %2691 : int = prim::Constant[value=3]() # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:46:0
  %2692 : int = aten::size(%input.1, %2691) # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:46:0
  %2693 : Long(device=cpu) = prim::NumToTensor(%2692)
  %2695 : int = aten::Int(%2693)
  %2696 : int[] = prim::ListConstruct(%2694, %2695)
  %2697 : bool = prim::Constant[value=1]() # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:3151:0
  %2698 : None = prim::Constant()
  %input : Float(2:262144, 1:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::upsample_bilinear2d(%3728, %2696, %2697, %2698) # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:3151:0
  %conf : Float(2:262144, 1:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::sigmoid(%input) # C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\functional.py:1640:0
  %2701 : Float(2:786432, 3:262144, 512:512, 512:1, requires_grad=1, device=cpu) = aten::mul(%pre_conf, %conf) # D:\00code\small_obstacle_discovery-master\modeling\deeplab.py:51:0
  %2702 : (Float(2:786432, 3:262144, 512:512, 512:1, requires_grad=1, device=cpu), Float(2:262144, 1:262144, 512:512, 512:1, requires_grad=1, device=cpu), Float(2:786432, 3:262144, 512:512, 512:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%2701, %conf, %pre_conf)
  return (%2702)

Starting Epoch: 0
Total Epoches: 20
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 0, learning rate = 0.0050,                 previous best = 0.0000
C:\ProgramData\Anaconda3\envs\t\lib\site-packages\torch\nn\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Train loss: 0.211: 100%|██████████| 518/518 [22:16<00:00,  2.58s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.503:   2%|▏         | 1/50 [00:01<01:07,  1.37s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.415:   4%|▍         | 2/50 [00:02<00:44,  1.07it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317:   6%|▌         | 3/50 [00:02<00:37,  1.26it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.281:   8%|▊         | 4/50 [00:03<00:33,  1.36it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.279:  10%|█         | 5/50 [00:03<00:31,  1.41it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.262:  12%|█▏        | 6/50 [00:04<00:30,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.269:  14%|█▍        | 7/50 [00:05<00:29,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.247:  16%|█▌        | 8/50 [00:05<00:28,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.242:  18%|█▊        | 9/50 [00:06<00:27,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.236:  20%|██        | 10/50 [00:07<00:26,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.242:  22%|██▏       | 11/50 [00:08<00:34,  1.13it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.241:  24%|██▍       | 12/50 [00:09<00:31,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.247:  26%|██▌       | 13/50 [00:09<00:28,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.243:  28%|██▊       | 14/50 [00:10<00:26,  1.36it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.262:  30%|███       | 15/50 [00:11<00:24,  1.41it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.253:  32%|███▏      | 16/50 [00:11<00:23,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.257:  34%|███▍      | 17/50 [00:12<00:22,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.253:  36%|███▌      | 18/50 [00:13<00:21,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.251:  38%|███▊      | 19/50 [00:13<00:20,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.251:  40%|████      | 20/50 [00:14<00:20,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  42%|████▏     | 21/50 [00:15<00:25,  1.12it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.266:  44%|████▍     | 22/50 [00:16<00:23,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  46%|████▌     | 23/50 [00:17<00:21,  1.27it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.279:  48%|████▊     | 24/50 [00:18<00:19,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.278:  50%|█████     | 25/50 [00:18<00:18,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.274:  52%|█████▏    | 26/50 [00:19<00:17,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.270:  54%|█████▍    | 27/50 [00:20<00:17,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.268:  56%|█████▌    | 28/50 [00:20<00:15,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.265:  58%|█████▊    | 29/50 [00:21<00:15,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  60%|██████    | 30/50 [00:22<00:14,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.258:  62%|██████▏   | 31/50 [00:23<00:18,  1.05it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.261:  64%|██████▍   | 32/50 [00:24<00:16,  1.12it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  66%|██████▌   | 33/50 [00:25<00:14,  1.18it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.262:  68%|██████▊   | 34/50 [00:26<00:12,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  70%|███████   | 35/50 [00:26<00:11,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  72%|███████▏  | 36/50 [00:27<00:10,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.266:  74%|███████▍  | 37/50 [00:28<00:09,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.267:  76%|███████▌  | 38/50 [00:28<00:08,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.265:  78%|███████▊  | 39/50 [00:29<00:08,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.264:  80%|████████  | 40/50 [00:30<00:07,  1.36it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.261:  82%|████████▏ | 41/50 [00:31<00:08,  1.03it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  84%|████████▍ | 42/50 [00:32<00:07,  1.09it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.258:  86%|████████▌ | 43/50 [00:33<00:06,  1.17it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.257:  88%|████████▊ | 44/50 [00:34<00:04,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.260:  90%|█████████ | 45/50 [00:34<00:04,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  92%|█████████▏| 46/50 [00:35<00:03,  1.26it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.258:  94%|█████████▍| 47/50 [00:36<00:02,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.257:  96%|█████████▌| 48/50 [00:37<00:01,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  98%|█████████▊| 49/50 [00:37<00:00,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.258: 100%|██████████| 50/50 [00:38<00:00,  1.29it/s]
(100, 512, 512)
Validation:
[Epoch: 0, numImages:   100]
Acc:0.8273315048217773, Acc_class:0.5721435824709741, mIoU:0.48457322274645503, fwIoU: 0.7281748697819195
Loss: 12.905
Recall/PDR:0.05689534382410002
Precision:0.004594442280618917
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 1, learning rate = 0.0048,                 previous best = 0.4846
Train loss: 0.167: 100%|██████████| 518/518 [21:17<00:00,  2.47s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.230:   2%|▏         | 1/50 [00:01<01:02,  1.27s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.219:   4%|▍         | 2/50 [00:01<00:40,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.231:   6%|▌         | 3/50 [00:02<00:32,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.308:   8%|▊         | 4/50 [00:02<00:28,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.276:  10%|█         | 5/50 [00:03<00:26,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.271:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.292:  14%|█▍        | 7/50 [00:04<00:23,  1.81it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.279:  16%|█▌        | 8/50 [00:04<00:22,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.266:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.251:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.260:  22%|██▏       | 11/50 [00:07<00:29,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.267:  24%|██▍       | 12/50 [00:07<00:26,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.253:  26%|██▌       | 13/50 [00:08<00:24,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.248:  28%|██▊       | 14/50 [00:08<00:22,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.244:  30%|███       | 15/50 [00:09<00:20,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.238:  32%|███▏      | 16/50 [00:10<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.230:  34%|███▍      | 17/50 [00:10<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.233:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.226:  38%|███▊      | 19/50 [00:11<00:17,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.230:  40%|████      | 20/50 [00:12<00:17,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.228:  42%|████▏     | 21/50 [00:13<00:22,  1.27it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.225:  44%|████▍     | 22/50 [00:14<00:20,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.253:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.248:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.256:  50%|█████     | 25/50 [00:15<00:15,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.254:  52%|█████▏    | 26/50 [00:16<00:14,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.256:  54%|█████▍    | 27/50 [00:17<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.257:  56%|█████▌    | 28/50 [00:17<00:13,  1.66it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.252:  58%|█████▊    | 29/50 [00:18<00:12,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.249:  60%|██████    | 30/50 [00:18<00:11,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.250:  62%|██████▏   | 31/50 [00:20<00:15,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.250:  64%|██████▍   | 32/50 [00:20<00:13,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.247:  66%|██████▌   | 33/50 [00:21<00:12,  1.41it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.244:  68%|██████▊   | 34/50 [00:21<00:10,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.241:  70%|███████   | 35/50 [00:22<00:09,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.239:  72%|███████▏  | 36/50 [00:23<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.240:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.236:  76%|███████▌  | 38/50 [00:24<00:07,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.238:  78%|███████▊  | 39/50 [00:24<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.240:  80%|████████  | 40/50 [00:25<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.240:  82%|████████▏ | 41/50 [00:27<00:07,  1.17it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.236:  84%|████████▍ | 42/50 [00:27<00:06,  1.27it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.234:  86%|████████▌ | 43/50 [00:28<00:05,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.231:  88%|████████▊ | 44/50 [00:28<00:04,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.230:  90%|█████████ | 45/50 [00:29<00:03,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.226:  92%|█████████▏| 46/50 [00:30<00:02,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.226:  94%|█████████▍| 47/50 [00:30<00:01,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.222:  96%|█████████▌| 48/50 [00:31<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.224:  98%|█████████▊| 49/50 [00:32<00:00,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.229: 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
(100, 512, 512)
Validation:
[Epoch: 1, numImages:   100]
Acc:0.8558455276489257, Acc_class:0.6467053029630772, mIoU:0.5176267158030313, fwIoU: 0.7651453524654566
Loss: 11.445
Recall/PDR:0.21974618143512828
Precision:0.02563438632832729
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 2, learning rate = 0.0045,                 previous best = 0.5176
Train loss: 0.146: 100%|██████████| 518/518 [21:16<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.112:   2%|▏         | 1/50 [00:01<01:00,  1.23s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.343:   4%|▍         | 2/50 [00:01<00:39,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.288:   6%|▌         | 3/50 [00:02<00:31,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:   8%|▊         | 4/50 [00:02<00:28,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.331:  10%|█         | 5/50 [00:03<00:26,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.302:  12%|█▏        | 6/50 [00:03<00:24,  1.79it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.270:  14%|█▍        | 7/50 [00:04<00:23,  1.81it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.275:  16%|█▌        | 8/50 [00:04<00:22,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.398:  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.393:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.386:  22%|██▏       | 11/50 [00:07<00:30,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.437:  24%|██▍       | 12/50 [00:07<00:26,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.417:  26%|██▌       | 13/50 [00:08<00:24,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.427:  28%|██▊       | 14/50 [00:08<00:22,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.413:  30%|███       | 15/50 [00:09<00:21,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.402:  32%|███▏      | 16/50 [00:10<00:20,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.387:  34%|███▍      | 17/50 [00:10<00:19,  1.73it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.379:  36%|███▌      | 18/50 [00:11<00:18,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.371:  38%|███▊      | 19/50 [00:11<00:17,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.375:  40%|████      | 20/50 [00:12<00:17,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.365:  42%|████▏     | 21/50 [00:13<00:22,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  44%|████▍     | 22/50 [00:14<00:20,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.359:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.375:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.403:  50%|█████     | 25/50 [00:15<00:15,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.397:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.405:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.396:  56%|█████▌    | 28/50 [00:17<00:13,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.390:  58%|█████▊    | 29/50 [00:18<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.419:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.410:  62%|██████▏   | 31/50 [00:20<00:15,  1.23it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.404:  64%|██████▍   | 32/50 [00:20<00:13,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.409:  66%|██████▌   | 33/50 [00:21<00:11,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.402:  68%|██████▊   | 34/50 [00:21<00:10,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.397:  70%|███████   | 35/50 [00:22<00:09,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.392:  72%|███████▏  | 36/50 [00:23<00:08,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.393:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.387:  76%|███████▌  | 38/50 [00:24<00:07,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.382:  78%|███████▊  | 39/50 [00:24<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.377:  80%|████████  | 40/50 [00:25<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  82%|████████▏ | 41/50 [00:26<00:07,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.371:  84%|████████▍ | 42/50 [00:27<00:06,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.371:  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.374:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.375:  90%|█████████ | 45/50 [00:29<00:03,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.371:  92%|█████████▏| 46/50 [00:30<00:02,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.376:  94%|█████████▍| 47/50 [00:30<00:01,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.392:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.388:  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.389: 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
(100, 512, 512)
Validation:
[Epoch: 2, numImages:   100]
Acc:0.7943352508544922, Acc_class:0.5751956906839701, mIoU:0.4512011425852449, fwIoU: 0.6693322668085357
Loss: 19.437
Recall/PDR:0.15971335931840916
Precision:0.026283821563490736
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 3, learning rate = 0.0043,                 previous best = 0.5176
Train loss: 0.138: 100%|██████████| 518/518 [21:16<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.390:   2%|▏         | 1/50 [00:01<01:00,  1.24s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.226:   4%|▍         | 2/50 [00:01<00:39,  1.21it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.188:   6%|▌         | 3/50 [00:02<00:32,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.307:   8%|▊         | 4/50 [00:02<00:28,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.297:  10%|█         | 5/50 [00:03<00:26,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.285:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.266:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.284:  16%|█▌        | 8/50 [00:04<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.253:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.267:  22%|██▏       | 11/50 [00:07<00:29,  1.33it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.296:  24%|██▍       | 12/50 [00:07<00:26,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.307:  26%|██▌       | 13/50 [00:08<00:23,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.312:  28%|██▊       | 14/50 [00:08<00:22,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.305:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.300:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.302:  34%|███▍      | 17/50 [00:10<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.293:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.305:  38%|███▊      | 19/50 [00:11<00:17,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.301:  40%|████      | 20/50 [00:12<00:16,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.315:  42%|████▏     | 21/50 [00:13<00:22,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.314:  44%|████▍     | 22/50 [00:13<00:20,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.311:  46%|████▌     | 23/50 [00:14<00:18,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.310:  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.307:  50%|█████     | 25/50 [00:15<00:15,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.300:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.298:  54%|█████▍    | 27/50 [00:16<00:13,  1.66it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.294:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.290:  58%|█████▊    | 29/50 [00:18<00:12,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.287:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.297:  62%|██████▏   | 31/50 [00:19<00:15,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.305:  64%|██████▍   | 32/50 [00:20<00:13,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  66%|██████▌   | 33/50 [00:21<00:12,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.310:  68%|██████▊   | 34/50 [00:21<00:10,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.306:  70%|███████   | 35/50 [00:22<00:09,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  72%|███████▏  | 36/50 [00:23<00:09,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  74%|███████▍  | 37/50 [00:23<00:08,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.306:  76%|███████▌  | 38/50 [00:24<00:07,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.303:  78%|███████▊  | 39/50 [00:24<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.297:  80%|████████  | 40/50 [00:25<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.301:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.298:  84%|████████▍ | 42/50 [00:27<00:06,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.294:  86%|████████▌ | 43/50 [00:28<00:05,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.293:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.308:  90%|█████████ | 45/50 [00:29<00:03,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  96%|█████████▌| 48/50 [00:31<00:01,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.291: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 3, numImages:   100]
Acc:0.8304561614990235, Acc_class:0.7250590375570966, mIoU:0.5121283063885947, fwIoU: 0.7386289835955172
Loss: 14.555
Recall/PDR:0.5165499579591916
Precision:0.05707260962927872
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 4, learning rate = 0.0041,                 previous best = 0.5176
Train loss: 0.124: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.538:   2%|▏         | 1/50 [00:01<01:00,  1.24s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.374:   4%|▍         | 2/50 [00:01<00:39,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.315:   6%|▌         | 3/50 [00:02<00:32,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.358:   8%|▊         | 4/50 [00:02<00:29,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.388:  10%|█         | 5/50 [00:03<00:26,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.348:  12%|█▏        | 6/50 [00:03<00:25,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.329:  14%|█▍        | 7/50 [00:04<00:24,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.314:  16%|█▌        | 8/50 [00:04<00:23,  1.80it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.314:  18%|█▊        | 9/50 [00:05<00:22,  1.83it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.306:  20%|██        | 10/50 [00:06<00:21,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  22%|██▏       | 11/50 [00:07<00:29,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.318:  24%|██▍       | 12/50 [00:07<00:26,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:  26%|██▌       | 13/50 [00:08<00:24,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.325:  28%|██▊       | 14/50 [00:08<00:22,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.386:  30%|███       | 15/50 [00:09<00:20,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  32%|███▏      | 16/50 [00:10<00:19,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.375:  34%|███▍      | 17/50 [00:10<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.362:  36%|███▌      | 18/50 [00:11<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.350:  38%|███▊      | 19/50 [00:11<00:17,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.339:  40%|████      | 20/50 [00:12<00:17,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  42%|████▏     | 21/50 [00:13<00:22,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  44%|████▍     | 22/50 [00:14<00:20,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.323:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.316:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.307:  50%|█████     | 25/50 [00:15<00:15,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.306:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  54%|█████▍    | 27/50 [00:17<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.297:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.292:  58%|█████▊    | 29/50 [00:18<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.287:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.288:  62%|██████▏   | 31/50 [00:20<00:15,  1.23it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.287:  64%|██████▍   | 32/50 [00:20<00:13,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.285:  66%|██████▌   | 33/50 [00:21<00:11,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.283:  68%|██████▊   | 34/50 [00:21<00:10,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.286:  70%|███████   | 35/50 [00:22<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.281:  72%|███████▏  | 36/50 [00:23<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.275:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.272:  76%|███████▌  | 38/50 [00:24<00:07,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.268:  78%|███████▊  | 39/50 [00:25<00:07,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.265:  80%|████████  | 40/50 [00:25<00:06,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.265:  82%|████████▏ | 41/50 [00:27<00:07,  1.17it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.260:  84%|████████▍ | 42/50 [00:27<00:06,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  86%|████████▌ | 43/50 [00:28<00:05,  1.36it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.256:  88%|████████▊ | 44/50 [00:28<00:04,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.262:  90%|█████████ | 45/50 [00:29<00:03,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.268:  92%|█████████▏| 46/50 [00:30<00:02,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.268:  94%|█████████▍| 47/50 [00:30<00:01,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.270:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.268:  98%|█████████▊| 49/50 [00:32<00:00,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.275: 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
(100, 512, 512)
Validation:
[Epoch: 4, numImages:   100]
Acc:0.8467998123168945, Acc_class:0.6666939690638544, mIoU:0.5270424933144002, fwIoU: 0.7362446156737646
Loss: 13.736
Recall/PDR:0.3190963008383639
Precision:0.1466917446709241
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 5, learning rate = 0.0039,                 previous best = 0.5270
Train loss: 0.113: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.458:   2%|▏         | 1/50 [00:01<01:00,  1.24s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.405:   4%|▍         | 2/50 [00:01<00:39,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.530:   6%|▌         | 3/50 [00:02<00:32,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.492:   8%|▊         | 4/50 [00:02<00:28,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.497:  10%|█         | 5/50 [00:03<00:26,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.517:  12%|█▏        | 6/50 [00:03<00:24,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.531:  14%|█▍        | 7/50 [00:04<00:23,  1.81it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.553:  16%|█▌        | 8/50 [00:04<00:22,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.517:  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.493:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.479:  22%|██▏       | 11/50 [00:07<00:29,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.462:  24%|██▍       | 12/50 [00:07<00:26,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.505:  26%|██▌       | 13/50 [00:08<00:24,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.516:  28%|██▊       | 14/50 [00:08<00:22,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.547:  30%|███       | 15/50 [00:09<00:20,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.548:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.553:  34%|███▍      | 17/50 [00:10<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.544:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.527:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.520:  40%|████      | 20/50 [00:12<00:16,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.503:  42%|████▏     | 21/50 [00:13<00:22,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.488:  44%|████▍     | 22/50 [00:14<00:19,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.482:  46%|████▌     | 23/50 [00:14<00:18,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.467:  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.495:  50%|█████     | 25/50 [00:15<00:15,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.489:  52%|█████▏    | 26/50 [00:16<00:14,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.498:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.500:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.496:  58%|█████▊    | 29/50 [00:18<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.485:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.491:  62%|██████▏   | 31/50 [00:19<00:15,  1.23it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.496:  64%|██████▍   | 32/50 [00:20<00:13,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.492:  66%|██████▌   | 33/50 [00:21<00:12,  1.41it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.485:  68%|██████▊   | 34/50 [00:21<00:10,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.483:  70%|███████   | 35/50 [00:22<00:09,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.495:  72%|███████▏  | 36/50 [00:23<00:08,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.491:  74%|███████▍  | 37/50 [00:23<00:08,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.486:  76%|███████▌  | 38/50 [00:24<00:07,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.478:  78%|███████▊  | 39/50 [00:24<00:06,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.479:  80%|████████  | 40/50 [00:25<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.473:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.471:  84%|████████▍ | 42/50 [00:27<00:06,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.465:  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.461:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.459:  90%|█████████ | 45/50 [00:29<00:03,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.458:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.453:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.467:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.471:  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.475: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 5, numImages:   100]
Acc:0.7512454986572266, Acc_class:0.5627401276307517, mIoU:0.4026462970888089, fwIoU: 0.6076740988079605
Loss: 23.761
Recall/PDR:0.24638138906738405
Precision:0.0246564801463127
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 6, learning rate = 0.0036,                 previous best = 0.5270
Train loss: 0.107: 100%|██████████| 518/518 [21:16<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.174:   2%|▏         | 1/50 [00:01<00:59,  1.21s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.234:   4%|▍         | 2/50 [00:01<00:38,  1.23it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.234:   6%|▌         | 3/50 [00:02<00:31,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.203:   8%|▊         | 4/50 [00:02<00:28,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.208:  10%|█         | 5/50 [00:03<00:26,  1.73it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.187:  12%|█▏        | 6/50 [00:03<00:24,  1.79it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.172:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.175:  16%|█▌        | 8/50 [00:04<00:22,  1.83it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.207:  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.197:  20%|██        | 10/50 [00:05<00:21,  1.87it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.190:  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.194:  24%|██▍       | 12/50 [00:07<00:26,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.205:  26%|██▌       | 13/50 [00:08<00:23,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.209:  28%|██▊       | 14/50 [00:08<00:22,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.201:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.202:  32%|███▏      | 16/50 [00:09<00:19,  1.73it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.209:  34%|███▍      | 17/50 [00:10<00:18,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.207:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.217:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.216:  40%|████      | 20/50 [00:12<00:16,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.212:  42%|████▏     | 21/50 [00:13<00:22,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.208:  44%|████▍     | 22/50 [00:13<00:20,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.206:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.209:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.226:  50%|█████     | 25/50 [00:15<00:15,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.226:  52%|█████▏    | 26/50 [00:16<00:14,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.229:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.233:  56%|█████▌    | 28/50 [00:17<00:13,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.239:  58%|█████▊    | 29/50 [00:18<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.242:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.240:  62%|██████▏   | 31/50 [00:19<00:15,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.238:  64%|██████▍   | 32/50 [00:20<00:13,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.243:  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.242:  68%|██████▊   | 34/50 [00:21<00:10,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.242:  70%|███████   | 35/50 [00:22<00:09,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.252:  72%|███████▏  | 36/50 [00:22<00:08,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.248:  74%|███████▍  | 37/50 [00:23<00:08,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.249:  76%|███████▌  | 38/50 [00:24<00:07,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.251:  78%|███████▊  | 39/50 [00:24<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.253:  80%|████████  | 40/50 [00:25<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.249:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  84%|████████▍ | 42/50 [00:27<00:06,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.254:  86%|████████▌ | 43/50 [00:27<00:05,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  88%|████████▊ | 44/50 [00:28<00:04,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  90%|█████████ | 45/50 [00:29<00:03,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.260:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.262:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.258:  98%|█████████▊| 49/50 [00:31<00:00,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.257: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 6, numImages:   100]
Acc:0.8391610717773438, Acc_class:0.7308054897350748, mIoU:0.5205363367060217, fwIoU: 0.7437966532851211
Loss: 12.856
Recall/PDR:0.5188762071992976
Precision:0.07744645146692812
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 7, learning rate = 0.0034,                 previous best = 0.5270
Train loss: 0.100: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.571:   2%|▏         | 1/50 [00:01<01:01,  1.26s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.424:   4%|▍         | 2/50 [00:01<00:39,  1.21it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.384:   6%|▌         | 3/50 [00:02<00:32,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.329:   8%|▊         | 4/50 [00:02<00:28,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  10%|█         | 5/50 [00:03<00:26,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.321:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.300:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.341:  16%|█▌        | 8/50 [00:04<00:22,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.370:  20%|██        | 10/50 [00:05<00:21,  1.87it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.377:  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.408:  24%|██▍       | 12/50 [00:07<00:26,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.407:  26%|██▌       | 13/50 [00:08<00:23,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.407:  28%|██▊       | 14/50 [00:08<00:22,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.404:  30%|███       | 15/50 [00:09<00:20,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.390:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.382:  34%|███▍      | 17/50 [00:10<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.379:  36%|███▌      | 18/50 [00:11<00:18,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.423:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.407:  40%|████      | 20/50 [00:12<00:16,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.395:  42%|████▏     | 21/50 [00:13<00:22,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.385:  44%|████▍     | 22/50 [00:13<00:19,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.385:  46%|████▌     | 23/50 [00:14<00:18,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.397:  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.392:  50%|█████     | 25/50 [00:15<00:15,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.394:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.396:  54%|█████▍    | 27/50 [00:16<00:13,  1.66it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.414:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.408:  58%|█████▊    | 29/50 [00:18<00:12,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.400:  60%|██████    | 30/50 [00:18<00:11,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.403:  62%|██████▏   | 31/50 [00:19<00:15,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.397:  64%|██████▍   | 32/50 [00:20<00:13,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.388:  66%|██████▌   | 33/50 [00:21<00:11,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.391:  68%|██████▊   | 34/50 [00:21<00:10,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.400:  70%|███████   | 35/50 [00:22<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.395:  72%|███████▏  | 36/50 [00:22<00:08,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.393:  74%|███████▍  | 37/50 [00:23<00:08,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.388:  76%|███████▌  | 38/50 [00:24<00:07,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.382:  78%|███████▊  | 39/50 [00:24<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.375:  80%|████████  | 40/50 [00:25<00:06,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.379:  82%|████████▏ | 41/50 [00:26<00:07,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.374:  84%|████████▍ | 42/50 [00:27<00:06,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.382:  86%|████████▌ | 43/50 [00:27<00:05,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.380:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.379:  90%|█████████ | 45/50 [00:29<00:03,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.371:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.366:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.364:  98%|█████████▊| 49/50 [00:31<00:00,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.376: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 7, numImages:   100]
Acc:0.7900848388671875, Acc_class:0.6553115109656915, mIoU:0.4600708383405403, fwIoU: 0.6791626021012416
Loss: 18.791
Recall/PDR:0.4116174504800097
Precision:0.03384472115474927
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 8, learning rate = 0.0032,                 previous best = 0.5270
Train loss: 0.094: 100%|██████████| 518/518 [21:16<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.161:   2%|▏         | 1/50 [00:01<01:01,  1.25s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.241:   4%|▍         | 2/50 [00:01<00:39,  1.21it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.243:   6%|▌         | 3/50 [00:02<00:32,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.284:   8%|▊         | 4/50 [00:02<00:28,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.366:  10%|█         | 5/50 [00:03<00:26,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.293:  14%|█▍        | 7/50 [00:04<00:23,  1.81it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.284:  16%|█▌        | 8/50 [00:04<00:22,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.298:  20%|██        | 10/50 [00:05<00:21,  1.87it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.300:  22%|██▏       | 11/50 [00:07<00:29,  1.33it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.289:  24%|██▍       | 12/50 [00:07<00:26,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.276:  26%|██▌       | 13/50 [00:08<00:23,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.283:  28%|██▊       | 14/50 [00:08<00:22,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.283:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.282:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.284:  34%|███▍      | 17/50 [00:10<00:18,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.278:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.277:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.289:  40%|████      | 20/50 [00:12<00:16,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.301:  42%|████▏     | 21/50 [00:13<00:22,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  44%|████▍     | 22/50 [00:13<00:19,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.288:  46%|████▌     | 23/50 [00:14<00:17,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.288:  48%|████▊     | 24/50 [00:15<00:16,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.296:  50%|█████     | 25/50 [00:15<00:15,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  52%|█████▏    | 26/50 [00:16<00:14,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.294:  54%|█████▍    | 27/50 [00:16<00:13,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.289:  56%|█████▌    | 28/50 [00:17<00:13,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.290:  58%|█████▊    | 29/50 [00:18<00:13,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.286:  60%|██████    | 30/50 [00:18<00:12,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.280:  62%|██████▏   | 31/50 [00:20<00:15,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.282:  64%|██████▍   | 32/50 [00:20<00:13,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.287:  66%|██████▌   | 33/50 [00:21<00:12,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.287:  68%|██████▊   | 34/50 [00:21<00:11,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  70%|███████   | 35/50 [00:22<00:10,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.293:  72%|███████▏  | 36/50 [00:23<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.289:  74%|███████▍  | 37/50 [00:23<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  76%|███████▌  | 38/50 [00:24<00:07,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  78%|███████▊  | 39/50 [00:24<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.313:  80%|████████  | 40/50 [00:25<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.312:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.311:  84%|████████▍ | 42/50 [00:27<00:06,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.306:  88%|████████▊ | 44/50 [00:28<00:04,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.302:  90%|█████████ | 45/50 [00:29<00:03,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.298:  92%|█████████▏| 46/50 [00:30<00:02,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  94%|█████████▍| 47/50 [00:30<00:02,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.300:  96%|█████████▌| 48/50 [00:31<00:01,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.298:  98%|█████████▊| 49/50 [00:32<00:00,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.305: 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
(100, 512, 512)
Validation:
[Epoch: 8, numImages:   100]
Acc:0.8044499969482422, Acc_class:0.7060348117427971, mIoU:0.47520532827606327, fwIoU: 0.6857534138712673
Loss: 15.253
Recall/PDR:0.5327871059220504
Precision:0.06447762685320639
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 9, learning rate = 0.0029,                 previous best = 0.5270
Train loss: 0.088: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.229:   2%|▏         | 1/50 [00:01<00:59,  1.21s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.217:   4%|▍         | 2/50 [00:01<00:38,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.188:   6%|▌         | 3/50 [00:02<00:31,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.244:   8%|▊         | 4/50 [00:02<00:28,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.266:  10%|█         | 5/50 [00:03<00:26,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.316:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.311:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.286:  16%|█▌        | 8/50 [00:04<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.315:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.296:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.282:  22%|██▏       | 11/50 [00:07<00:28,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.264:  24%|██▍       | 12/50 [00:07<00:26,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.275:  26%|██▌       | 13/50 [00:08<00:23,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.288:  28%|██▊       | 14/50 [00:08<00:22,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.278:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.287:  32%|███▏      | 16/50 [00:09<00:19,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.287:  34%|███▍      | 17/50 [00:10<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  36%|███▌      | 18/50 [00:10<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.331:  40%|████      | 20/50 [00:12<00:16,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.332:  42%|████▏     | 21/50 [00:13<00:22,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.323:  44%|████▍     | 22/50 [00:13<00:19,  1.41it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324:  46%|████▌     | 23/50 [00:14<00:17,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.331:  48%|████▊     | 24/50 [00:15<00:16,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.331:  50%|█████     | 25/50 [00:15<00:15,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.350:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.344:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  58%|█████▊    | 29/50 [00:17<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.327:  62%|██████▏   | 31/50 [00:19<00:15,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.340:  64%|██████▍   | 32/50 [00:20<00:13,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.340:  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  68%|██████▊   | 34/50 [00:21<00:10,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  70%|███████   | 35/50 [00:22<00:09,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  72%|███████▏  | 36/50 [00:22<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.321:  76%|███████▌  | 38/50 [00:24<00:07,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:  78%|███████▊  | 39/50 [00:24<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:  80%|████████  | 40/50 [00:25<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.323:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  84%|████████▍ | 42/50 [00:27<00:06,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  86%|████████▌ | 43/50 [00:27<00:05,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.326:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:  90%|█████████ | 45/50 [00:29<00:03,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.320:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.326:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.329: 100%|██████████| 50/50 [00:32<00:00,  1.55it/s]
(100, 512, 512)
Validation:
[Epoch: 9, numImages:   100]
Acc:0.8111835098266602, Acc_class:0.6649702136690148, mIoU:0.4828180291736397, fwIoU: 0.7198845360973742
Loss: 16.459
Recall/PDR:0.39728340539236606
Precision:0.024161035956609442
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 10, learning rate = 0.0027,                 previous best = 0.5270
Train loss: 0.080: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.514:   2%|▏         | 1/50 [00:01<01:00,  1.23s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.432:   4%|▍         | 2/50 [00:01<00:39,  1.21it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.321:   6%|▌         | 3/50 [00:02<00:32,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:   8%|▊         | 4/50 [00:02<00:28,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.243:  10%|█         | 5/50 [00:03<00:26,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.245:  12%|█▏        | 6/50 [00:03<00:25,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.248:  14%|█▍        | 7/50 [00:04<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  16%|█▌        | 8/50 [00:04<00:23,  1.81it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.259:  18%|█▊        | 9/50 [00:05<00:22,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.246:  20%|██        | 10/50 [00:06<00:21,  1.83it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.235:  22%|██▏       | 11/50 [00:07<00:29,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.227:  24%|██▍       | 12/50 [00:07<00:26,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.290:  26%|██▌       | 13/50 [00:08<00:24,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.282:  28%|██▊       | 14/50 [00:08<00:22,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.276:  30%|███       | 15/50 [00:09<00:21,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.272:  32%|███▏      | 16/50 [00:10<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.273:  34%|███▍      | 17/50 [00:10<00:19,  1.70it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.269:  36%|███▌      | 18/50 [00:11<00:18,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.273:  38%|███▊      | 19/50 [00:11<00:17,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.264:  40%|████      | 20/50 [00:12<00:17,  1.73it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.258:  42%|████▏     | 21/50 [00:13<00:22,  1.26it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  44%|████▍     | 22/50 [00:14<00:20,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.290:  46%|████▌     | 23/50 [00:14<00:18,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.283:  48%|████▊     | 24/50 [00:15<00:17,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  50%|█████     | 25/50 [00:16<00:15,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.308:  52%|█████▏    | 26/50 [00:16<00:14,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.311:  54%|█████▍    | 27/50 [00:17<00:14,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.327:  56%|█████▌    | 28/50 [00:17<00:13,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.319:  58%|█████▊    | 29/50 [00:18<00:12,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.325:  60%|██████    | 30/50 [00:18<00:12,  1.66it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  62%|██████▏   | 31/50 [00:20<00:15,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.327:  64%|██████▍   | 32/50 [00:20<00:13,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.344:  66%|██████▌   | 33/50 [00:21<00:12,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.341:  68%|██████▊   | 34/50 [00:22<00:10,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.339:  70%|███████   | 35/50 [00:22<00:09,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  72%|███████▏  | 36/50 [00:23<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.351:  74%|███████▍  | 37/50 [00:24<00:08,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  76%|███████▌  | 38/50 [00:24<00:07,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.352:  78%|███████▊  | 39/50 [00:25<00:06,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.356:  80%|████████  | 40/50 [00:25<00:06,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.360:  82%|████████▏ | 41/50 [00:27<00:07,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.361:  84%|████████▍ | 42/50 [00:27<00:06,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.377:  86%|████████▌ | 43/50 [00:28<00:05,  1.36it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.372:  88%|████████▊ | 44/50 [00:29<00:04,  1.41it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.366:  90%|█████████ | 45/50 [00:29<00:03,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.371:  92%|█████████▏| 46/50 [00:30<00:02,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.368:  94%|█████████▍| 47/50 [00:31<00:02,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.363:  96%|█████████▌| 48/50 [00:31<00:01,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.364:  98%|█████████▊| 49/50 [00:32<00:00,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.368: 100%|██████████| 50/50 [00:33<00:00,  1.51it/s]
(100, 512, 512)
Validation:
[Epoch: 10, numImages:   100]
Acc:0.7973847961425782, Acc_class:0.6836026860028045, mIoU:0.46551986174047316, fwIoU: 0.671392655126068
Loss: 18.378
Recall/PDR:0.4999229761996457
Precision:0.07667725967842924
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 11, learning rate = 0.0024,                 previous best = 0.5270
Train loss: 0.080: 100%|██████████| 518/518 [21:22<00:00,  2.48s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.212:   2%|▏         | 1/50 [00:01<00:58,  1.19s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.374:   4%|▍         | 2/50 [00:01<00:38,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:   6%|▌         | 3/50 [00:02<00:31,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.293:   8%|▊         | 4/50 [00:02<00:28,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.273:  10%|█         | 5/50 [00:03<00:26,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.271:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.272:  14%|█▍        | 7/50 [00:04<00:23,  1.81it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.256:  16%|█▌        | 8/50 [00:04<00:22,  1.83it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.254:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.348:  24%|██▍       | 12/50 [00:07<00:26,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  26%|██▌       | 13/50 [00:08<00:24,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.416:  28%|██▊       | 14/50 [00:08<00:22,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.413:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.392:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.384:  34%|███▍      | 17/50 [00:10<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.378:  36%|███▌      | 18/50 [00:10<00:18,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.362:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.384:  40%|████      | 20/50 [00:12<00:16,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  42%|████▏     | 21/50 [00:13<00:22,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.365:  44%|████▍     | 22/50 [00:13<00:19,  1.41it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.361:  46%|████▌     | 23/50 [00:14<00:18,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.365:  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.352:  50%|█████     | 25/50 [00:15<00:15,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  52%|█████▏    | 26/50 [00:16<00:14,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.364:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.357:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.349:  58%|█████▊    | 29/50 [00:17<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  60%|██████    | 30/50 [00:18<00:11,  1.70it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.353:  62%|██████▏   | 31/50 [00:19<00:15,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.361:  64%|██████▍   | 32/50 [00:20<00:13,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.356:  66%|██████▌   | 33/50 [00:21<00:11,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.356:  68%|██████▊   | 34/50 [00:21<00:10,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.352:  70%|███████   | 35/50 [00:22<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.357:  72%|███████▏  | 36/50 [00:22<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.352:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  76%|███████▌  | 38/50 [00:24<00:07,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  78%|███████▊  | 39/50 [00:24<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  80%|████████  | 40/50 [00:25<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.358:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.352:  84%|████████▍ | 42/50 [00:27<00:06,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  86%|████████▌ | 43/50 [00:27<00:05,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.343:  88%|████████▊ | 44/50 [00:28<00:04,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.340:  90%|█████████ | 45/50 [00:29<00:03,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.348:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.343:  98%|█████████▊| 49/50 [00:31<00:00,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.342: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 11, numImages:   100]
Acc:0.8121654891967773, Acc_class:0.7579844202771518, mIoU:0.4930679928725799, fwIoU: 0.7037107957232863
Loss: 17.117
Recall/PDR:0.6728930834642257
Precision:0.08178858684292385
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 12, learning rate = 0.0022,                 previous best = 0.5270
Train loss: 0.074: 100%|██████████| 518/518 [21:14<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.369:   2%|▏         | 1/50 [00:01<00:58,  1.18s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.329:   4%|▍         | 2/50 [00:01<00:38,  1.26it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:   6%|▌         | 3/50 [00:02<00:31,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.464:   8%|▊         | 4/50 [00:02<00:28,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.404:  10%|█         | 5/50 [00:03<00:25,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.359:  12%|█▏        | 6/50 [00:03<00:24,  1.80it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  14%|█▍        | 7/50 [00:04<00:23,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.313:  16%|█▌        | 8/50 [00:04<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.300:  18%|█▊        | 9/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.293:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.279:  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.270:  24%|██▍       | 12/50 [00:07<00:25,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.276:  26%|██▌       | 13/50 [00:08<00:23,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.275:  28%|██▊       | 14/50 [00:08<00:21,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.273:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.275:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.266:  34%|███▍      | 17/50 [00:10<00:18,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  36%|███▌      | 18/50 [00:10<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.261:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.276:  40%|████      | 20/50 [00:12<00:16,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.270:  42%|████▏     | 21/50 [00:13<00:22,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.265:  44%|████▍     | 22/50 [00:13<00:20,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.261:  46%|████▌     | 23/50 [00:14<00:18,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.257:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.265:  50%|█████     | 25/50 [00:15<00:15,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.305:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.298:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  56%|█████▌    | 28/50 [00:17<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.289:  58%|█████▊    | 29/50 [00:17<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.284:  60%|██████    | 30/50 [00:18<00:11,  1.70it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.281:  62%|██████▏   | 31/50 [00:19<00:15,  1.25it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.275:  64%|██████▍   | 32/50 [00:20<00:13,  1.36it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.273:  66%|██████▌   | 33/50 [00:21<00:11,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.276:  68%|██████▊   | 34/50 [00:21<00:10,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.272:  70%|███████   | 35/50 [00:22<00:09,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.267:  72%|███████▏  | 36/50 [00:22<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.263:  76%|███████▌  | 38/50 [00:24<00:07,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.266:  78%|███████▊  | 39/50 [00:24<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.262:  80%|████████  | 40/50 [00:25<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.278:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.278:  84%|████████▍ | 42/50 [00:27<00:06,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.274:  86%|████████▌ | 43/50 [00:27<00:05,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.277:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.278:  90%|█████████ | 45/50 [00:29<00:03,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.296:  92%|█████████▏| 46/50 [00:29<00:02,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.300:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.292:  98%|█████████▊| 49/50 [00:31<00:00,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.296: 100%|██████████| 50/50 [00:32<00:00,  1.55it/s]
(100, 512, 512)
Validation:
[Epoch: 12, numImages:   100]
Acc:0.8370620727539062, Acc_class:0.7106507377135264, mIoU:0.5078815049651384, fwIoU: 0.7450047486670358
Loss: 14.822
Recall/PDR:0.4652108799481907
Precision:0.04032138531418914
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 13, learning rate = 0.0019,                 previous best = 0.5270
Train loss: 0.072: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.154:   2%|▏         | 1/50 [00:01<00:59,  1.21s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.372:   4%|▍         | 2/50 [00:01<00:38,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.383:   6%|▌         | 3/50 [00:02<00:31,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.315:   8%|▊         | 4/50 [00:02<00:28,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.378:  10%|█         | 5/50 [00:03<00:26,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.381:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.359:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  16%|█▌        | 8/50 [00:04<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.368:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.353:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  22%|██▏       | 11/50 [00:07<00:29,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.370:  24%|██▍       | 12/50 [00:07<00:26,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.357:  26%|██▌       | 13/50 [00:08<00:23,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.348:  28%|██▊       | 14/50 [00:08<00:22,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.331:  32%|███▏      | 16/50 [00:09<00:19,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.342:  34%|███▍      | 17/50 [00:10<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  36%|███▌      | 18/50 [00:11<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.342:  38%|███▊      | 19/50 [00:11<00:17,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  40%|████      | 20/50 [00:12<00:17,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  42%|████▏     | 21/50 [00:13<00:22,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  44%|████▍     | 22/50 [00:14<00:20,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.325:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.321:  50%|█████     | 25/50 [00:15<00:15,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.363:  54%|█████▍    | 27/50 [00:16<00:13,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.372:  56%|█████▌    | 28/50 [00:17<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.376:  58%|█████▊    | 29/50 [00:18<00:12,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.381:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.379:  62%|██████▏   | 31/50 [00:19<00:15,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.382:  64%|██████▍   | 32/50 [00:20<00:13,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.400:  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.398:  68%|██████▊   | 34/50 [00:21<00:10,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.394:  70%|███████   | 35/50 [00:22<00:09,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.392:  72%|███████▏  | 36/50 [00:22<00:08,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.395:  74%|███████▍  | 37/50 [00:23<00:08,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.416:  76%|███████▌  | 38/50 [00:24<00:07,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.425:  78%|███████▊  | 39/50 [00:24<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.427:  80%|████████  | 40/50 [00:25<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.425:  82%|████████▏ | 41/50 [00:26<00:07,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.420:  84%|████████▍ | 42/50 [00:27<00:06,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.416:  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.412:  88%|████████▊ | 44/50 [00:28<00:04,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.406:  90%|█████████ | 45/50 [00:29<00:03,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.402:  92%|█████████▏| 46/50 [00:29<00:02,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.398:  94%|█████████▍| 47/50 [00:30<00:01,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.393:  96%|█████████▌| 48/50 [00:31<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.413:  98%|█████████▊| 49/50 [00:31<00:00,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.412: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 13, numImages:   100]
Acc:0.7812156295776367, Acc_class:0.6876049773423949, mIoU:0.44646532017639956, fwIoU: 0.645637816555365
Loss: 20.600
Recall/PDR:0.5538915945540077
Precision:0.07496213154753684
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 14, learning rate = 0.0017,                 previous best = 0.5270
Train loss: 0.068: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.630:   2%|▏         | 1/50 [00:01<01:01,  1.25s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.568:   4%|▍         | 2/50 [00:01<00:39,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.750:   6%|▌         | 3/50 [00:02<00:32,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.589:   8%|▊         | 4/50 [00:02<00:28,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.508:  10%|█         | 5/50 [00:03<00:26,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.432:  12%|█▏        | 6/50 [00:03<00:24,  1.79it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.433:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.408:  16%|█▌        | 8/50 [00:04<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.396:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.363:  20%|██        | 10/50 [00:05<00:21,  1.87it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.363:  22%|██▏       | 11/50 [00:07<00:29,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.354:  24%|██▍       | 12/50 [00:07<00:26,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  26%|██▌       | 13/50 [00:08<00:23,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  28%|██▊       | 14/50 [00:08<00:22,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.352:  30%|███       | 15/50 [00:09<00:20,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.359:  32%|███▏      | 16/50 [00:09<00:19,  1.73it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  34%|███▍      | 17/50 [00:10<00:18,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  38%|███▊      | 19/50 [00:11<00:17,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  40%|████      | 20/50 [00:12<00:16,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  42%|████▏     | 21/50 [00:13<00:22,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.340:  44%|████▍     | 22/50 [00:13<00:20,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.318:  50%|█████     | 25/50 [00:15<00:15,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.312:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.307:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.315:  58%|█████▊    | 29/50 [00:18<00:12,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.332:  62%|██████▏   | 31/50 [00:19<00:15,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.327:  64%|██████▍   | 32/50 [00:20<00:13,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.344:  68%|██████▊   | 34/50 [00:21<00:10,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  70%|███████   | 35/50 [00:22<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  72%|███████▏  | 36/50 [00:22<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  74%|███████▍  | 37/50 [00:23<00:08,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  76%|███████▌  | 38/50 [00:24<00:07,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  78%|███████▊  | 39/50 [00:24<00:06,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.344:  80%|████████  | 40/50 [00:25<00:06,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  82%|████████▏ | 41/50 [00:26<00:07,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.332:  84%|████████▍ | 42/50 [00:27<00:06,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.329:  86%|████████▌ | 43/50 [00:28<00:05,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.329:  88%|████████▊ | 44/50 [00:28<00:04,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.338:  90%|█████████ | 45/50 [00:29<00:03,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.326:  96%|█████████▌| 48/50 [00:31<00:01,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.323:  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 14, numImages:   100]
Acc:0.8357223892211914, Acc_class:0.7486052643295148, mIoU:0.5149365382466291, fwIoU: 0.7261894000270354
Loss: 16.181
Recall/PDR:0.6396261820695786
Precision:0.1389424114798918
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 15, learning rate = 0.0014,                 previous best = 0.5270
Train loss: 0.064: 100%|██████████| 518/518 [21:15<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.203:   2%|▏         | 1/50 [00:01<00:58,  1.19s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.145:   4%|▍         | 2/50 [00:01<00:38,  1.23it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.131:   6%|▌         | 3/50 [00:02<00:31,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.128:   8%|▊         | 4/50 [00:02<00:28,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.147:  10%|█         | 5/50 [00:03<00:26,  1.73it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.150:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.338:  16%|█▌        | 8/50 [00:04<00:22,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.313:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.325:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.306:  22%|██▏       | 11/50 [00:07<00:29,  1.33it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.379:  24%|██▍       | 12/50 [00:07<00:26,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.399:  26%|██▌       | 13/50 [00:08<00:23,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.409:  28%|██▊       | 14/50 [00:08<00:22,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.398:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.386:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.369:  34%|███▍      | 17/50 [00:10<00:18,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.374:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.381:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.367:  40%|████      | 20/50 [00:12<00:16,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.364:  42%|████▏     | 21/50 [00:13<00:22,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.363:  44%|████▍     | 22/50 [00:13<00:20,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.358:  46%|████▌     | 23/50 [00:14<00:18,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.358:  50%|█████     | 25/50 [00:15<00:15,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.341:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  56%|█████▌    | 28/50 [00:17<00:13,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  58%|█████▊    | 29/50 [00:18<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.336:  60%|██████    | 30/50 [00:18<00:11,  1.70it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  62%|██████▏   | 31/50 [00:19<00:15,  1.26it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.327:  64%|██████▍   | 32/50 [00:20<00:13,  1.36it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:  66%|██████▌   | 33/50 [00:21<00:11,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317:  68%|██████▊   | 34/50 [00:21<00:10,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.312:  70%|███████   | 35/50 [00:22<00:09,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.342:  72%|███████▏  | 36/50 [00:22<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.359:  76%|███████▌  | 38/50 [00:24<00:07,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.352:  78%|███████▊  | 39/50 [00:24<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  80%|████████  | 40/50 [00:25<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.348:  82%|████████▏ | 41/50 [00:26<00:07,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.344:  84%|████████▍ | 42/50 [00:27<00:06,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.339:  86%|████████▌ | 43/50 [00:27<00:05,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.335:  88%|████████▊ | 44/50 [00:28<00:04,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  90%|█████████ | 45/50 [00:29<00:03,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  92%|█████████▏| 46/50 [00:29<00:02,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  94%|█████████▍| 47/50 [00:30<00:01,  1.52it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.321:  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 15, numImages:   100]
Acc:0.8371229553222657, Acc_class:0.6921165827642155, mIoU:0.5098532242326089, fwIoU: 0.7262582396984195
Loss: 15.835
Recall/PDR:0.4246869752103509
Precision:0.09430477088469653
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 16, learning rate = 0.0012,                 previous best = 0.5270
Train loss: 0.062: 100%|██████████| 518/518 [21:16<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.114:   2%|▏         | 1/50 [00:01<01:01,  1.25s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.080:   4%|▍         | 2/50 [00:01<00:39,  1.21it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:   6%|▌         | 3/50 [00:02<00:32,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.250:   8%|▊         | 4/50 [00:02<00:28,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.254:  10%|█         | 5/50 [00:03<00:26,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.251:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.241:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.284:  16%|█▌        | 8/50 [00:04<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.271:  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  20%|██        | 10/50 [00:05<00:21,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  22%|██▏       | 11/50 [00:07<00:28,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.326:  24%|██▍       | 12/50 [00:07<00:25,  1.46it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.311:  26%|██▌       | 13/50 [00:08<00:23,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.303:  28%|██▊       | 14/50 [00:08<00:21,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.351:  30%|███       | 15/50 [00:09<00:20,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.345:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  34%|███▍      | 17/50 [00:10<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324:  36%|███▌      | 18/50 [00:11<00:18,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.311:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317:  40%|████      | 20/50 [00:12<00:16,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  42%|████▏     | 21/50 [00:13<00:22,  1.27it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  44%|████▍     | 22/50 [00:14<00:20,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.289:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.283:  50%|█████     | 25/50 [00:15<00:15,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.282:  52%|█████▏    | 26/50 [00:16<00:14,  1.64it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.305:  54%|█████▍    | 27/50 [00:16<00:13,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  56%|█████▌    | 28/50 [00:17<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.310:  58%|█████▊    | 29/50 [00:18<00:12,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  60%|██████    | 30/50 [00:18<00:11,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  62%|██████▏   | 31/50 [00:19<00:15,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.314:  64%|██████▍   | 32/50 [00:20<00:13,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317:  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.313:  68%|██████▊   | 34/50 [00:21<00:10,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  70%|███████   | 35/50 [00:22<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.336:  72%|███████▏  | 36/50 [00:22<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  74%|███████▍  | 37/50 [00:23<00:08,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.344:  76%|███████▌  | 38/50 [00:24<00:07,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  78%|███████▊  | 39/50 [00:24<00:06,  1.58it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  80%|████████  | 40/50 [00:25<00:06,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.324:  82%|████████▏ | 41/50 [00:26<00:07,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  84%|████████▍ | 42/50 [00:27<00:06,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.359:  86%|████████▌ | 43/50 [00:28<00:05,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.366:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.366:  90%|█████████ | 45/50 [00:29<00:03,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.389:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.386:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.380:  96%|█████████▌| 48/50 [00:31<00:01,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.378:  98%|█████████▊| 49/50 [00:31<00:00,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.374: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 16, numImages:   100]
Acc:0.8378259658813476, Acc_class:0.6814662698011222, mIoU:0.5174034658494002, fwIoU: 0.725497020837452
Loss: 18.698
Recall/PDR:0.38520192418286503
Precision:0.12318923679103008
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 17, learning rate = 0.0009,                 previous best = 0.5270
Train loss: 0.062: 100%|██████████| 518/518 [21:16<00:00,  2.46s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.206:   2%|▏         | 1/50 [00:01<01:00,  1.23s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.165:   4%|▍         | 2/50 [00:01<00:39,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.135:   6%|▌         | 3/50 [00:02<00:32,  1.47it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.267:   8%|▊         | 4/50 [00:02<00:28,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.333:  10%|█         | 5/50 [00:03<00:26,  1.71it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.290:  12%|█▏        | 6/50 [00:03<00:24,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.269:  14%|█▍        | 7/50 [00:04<00:23,  1.81it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.330:  16%|█▌        | 8/50 [00:04<00:22,  1.84it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.313:  18%|█▊        | 9/50 [00:05<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.323:  20%|██        | 10/50 [00:05<00:21,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.310:  22%|██▏       | 11/50 [00:07<00:29,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  24%|██▍       | 12/50 [00:07<00:26,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.318:  26%|██▌       | 13/50 [00:08<00:24,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.329:  28%|██▊       | 14/50 [00:08<00:22,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.326:  30%|███       | 15/50 [00:09<00:20,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.343:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.337:  34%|███▍      | 17/50 [00:10<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.357:  36%|███▌      | 18/50 [00:11<00:18,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.372:  38%|███▊      | 19/50 [00:11<00:17,  1.75it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.383:  40%|████      | 20/50 [00:12<00:16,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.397:  42%|████▏     | 21/50 [00:13<00:22,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.383:  44%|████▍     | 22/50 [00:14<00:20,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.384:  46%|████▌     | 23/50 [00:14<00:18,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.379:  48%|████▊     | 24/50 [00:15<00:16,  1.55it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.434:  50%|█████     | 25/50 [00:15<00:15,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.423:  52%|█████▏    | 26/50 [00:16<00:14,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.410:  54%|█████▍    | 27/50 [00:16<00:13,  1.66it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.402:  56%|█████▌    | 28/50 [00:17<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.399:  58%|█████▊    | 29/50 [00:18<00:12,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.402:  60%|██████    | 30/50 [00:18<00:11,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.394:  62%|██████▏   | 31/50 [00:20<00:15,  1.25it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.396:  64%|██████▍   | 32/50 [00:20<00:13,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.389:  66%|██████▌   | 33/50 [00:21<00:11,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.385:  68%|██████▊   | 34/50 [00:21<00:10,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.385:  70%|███████   | 35/50 [00:22<00:09,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.377:  72%|███████▏  | 36/50 [00:23<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.378:  74%|███████▍  | 37/50 [00:23<00:08,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.372:  76%|███████▌  | 38/50 [00:24<00:07,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.364:  78%|███████▊  | 39/50 [00:24<00:06,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.359:  80%|████████  | 40/50 [00:25<00:06,  1.59it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.354:  82%|████████▏ | 41/50 [00:26<00:07,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  84%|████████▍ | 42/50 [00:27<00:06,  1.27it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.374:  86%|████████▌ | 43/50 [00:28<00:05,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  88%|████████▊ | 44/50 [00:28<00:04,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.367:  90%|█████████ | 45/50 [00:29<00:03,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.362:  92%|█████████▏| 46/50 [00:30<00:02,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.357:  94%|█████████▍| 47/50 [00:30<00:02,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.354:  96%|█████████▌| 48/50 [00:31<00:01,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.351:  98%|█████████▊| 49/50 [00:32<00:00,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.350: 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]
(100, 512, 512)
Validation:
[Epoch: 17, numImages:   100]
Acc:0.8298018264770508, Acc_class:0.6522402920206731, mIoU:0.49151799749161557, fwIoU: 0.7241793240574085
Loss: 17.483
Recall/PDR:0.3187330767395296
Precision:0.03733485315419539
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 18, learning rate = 0.0006,                 previous best = 0.5270
Train loss: 0.059: 100%|██████████| 518/518 [21:24<00:00,  2.48s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.222:   2%|▏         | 1/50 [00:01<01:13,  1.51s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.311:   4%|▍         | 2/50 [00:02<00:53,  1.11s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.380:   6%|▌         | 3/50 [00:03<00:44,  1.05it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.322:   8%|▊         | 4/50 [00:03<00:39,  1.17it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.296:  10%|█         | 5/50 [00:04<00:37,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.340:  12%|█▏        | 6/50 [00:05<00:36,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317:  14%|█▍        | 7/50 [00:06<00:32,  1.31it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  16%|█▌        | 8/50 [00:06<00:31,  1.33it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  18%|█▊        | 9/50 [00:07<00:31,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  20%|██        | 10/50 [00:08<00:29,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.334:  22%|██▏       | 11/50 [00:09<00:38,  1.01it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  24%|██▍       | 12/50 [00:10<00:34,  1.10it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.326:  26%|██▌       | 13/50 [00:11<00:32,  1.13it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.325:  28%|██▊       | 14/50 [00:12<00:30,  1.19it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.310:  30%|███       | 15/50 [00:12<00:27,  1.27it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  32%|███▏      | 16/50 [00:13<00:25,  1.33it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.310:  34%|███▍      | 17/50 [00:14<00:23,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317:  36%|███▌      | 18/50 [00:14<00:22,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.325:  38%|███▊      | 19/50 [00:15<00:21,  1.42it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  40%|████      | 20/50 [00:16<00:20,  1.45it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.328:  42%|████▏     | 21/50 [00:17<00:26,  1.10it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.317:  44%|████▍     | 22/50 [00:18<00:23,  1.18it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.312:  46%|████▌     | 23/50 [00:19<00:22,  1.22it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.308:  48%|████▊     | 24/50 [00:19<00:20,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.305:  50%|█████     | 25/50 [00:20<00:18,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  52%|█████▏    | 26/50 [00:21<00:17,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.315:  54%|█████▍    | 27/50 [00:21<00:16,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.327:  56%|█████▌    | 28/50 [00:22<00:15,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.321:  58%|█████▊    | 29/50 [00:23<00:15,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.357:  60%|██████    | 30/50 [00:23<00:14,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.347:  62%|██████▏   | 31/50 [00:25<00:17,  1.08it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.346:  64%|██████▍   | 32/50 [00:26<00:15,  1.17it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.370:  66%|██████▌   | 33/50 [00:26<00:13,  1.23it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.363:  68%|██████▊   | 34/50 [00:27<00:12,  1.28it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.375:  70%|███████   | 35/50 [00:28<00:11,  1.33it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.375:  72%|███████▏  | 36/50 [00:28<00:10,  1.35it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.371:  74%|███████▍  | 37/50 [00:29<00:09,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.373:  76%|███████▌  | 38/50 [00:30<00:08,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.369:  78%|███████▊  | 39/50 [00:30<00:07,  1.39it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.362:  80%|████████  | 40/50 [00:31<00:07,  1.37it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.383:  82%|████████▏ | 41/50 [00:33<00:08,  1.06it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.389:  84%|████████▍ | 42/50 [00:33<00:06,  1.14it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.389:  86%|████████▌ | 43/50 [00:34<00:05,  1.20it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.401:  88%|████████▊ | 44/50 [00:35<00:04,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.396:  90%|█████████ | 45/50 [00:36<00:03,  1.27it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.388:  92%|█████████▏| 46/50 [00:36<00:03,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.386:  94%|█████████▍| 47/50 [00:37<00:02,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.381:  96%|█████████▌| 48/50 [00:38<00:01,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.380:  98%|█████████▊| 49/50 [00:39<00:00,  1.33it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.377: 100%|██████████| 50/50 [00:39<00:00,  1.25it/s]
(100, 512, 512)
Validation:
[Epoch: 18, numImages:   100]
Acc:0.8174121475219727, Acc_class:0.6794782832578092, mIoU:0.4779767634719829, fwIoU: 0.6994200528870259
Loss: 18.848
Recall/PDR:0.4536968396410456
Precision:0.061664938763280774
training:   0%|          | 0/518 [00:00<?, ?it/s]
=>Epoches 19, learning rate = 0.0003,                 previous best = 0.5270
Train loss: 0.054: 100%|██████████| 518/518 [21:28<00:00,  2.49s/it]
(1036, 512, 512)
validation:   0%|          | 0/50 [00:00<?, ?it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.047:   2%|▏         | 1/50 [00:01<00:58,  1.20s/it]torch.Size([2, 3, 512, 512])
Test loss: 0.119:   4%|▍         | 2/50 [00:01<00:38,  1.24it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.110:   6%|▌         | 3/50 [00:02<00:31,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.108:   8%|▊         | 4/50 [00:02<00:28,  1.63it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.103:  10%|█         | 5/50 [00:03<00:26,  1.73it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.114:  12%|█▏        | 6/50 [00:03<00:24,  1.78it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.112:  14%|█▍        | 7/50 [00:04<00:23,  1.82it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.128:  16%|█▌        | 8/50 [00:04<00:22,  1.85it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.125:  18%|█▊        | 9/50 [00:05<00:22,  1.86it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.160:  20%|██        | 10/50 [00:05<00:21,  1.87it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.154:  22%|██▏       | 11/50 [00:07<00:29,  1.32it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.235:  24%|██▍       | 12/50 [00:07<00:26,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.242:  26%|██▌       | 13/50 [00:08<00:23,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  28%|██▊       | 14/50 [00:08<00:22,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.244:  30%|███       | 15/50 [00:09<00:20,  1.68it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.239:  32%|███▏      | 16/50 [00:09<00:19,  1.72it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.235:  34%|███▍      | 17/50 [00:10<00:18,  1.74it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.230:  36%|███▌      | 18/50 [00:11<00:18,  1.76it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.262:  38%|███▊      | 19/50 [00:11<00:17,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.260:  40%|████      | 20/50 [00:12<00:16,  1.77it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.255:  42%|████▏     | 21/50 [00:13<00:22,  1.29it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.271:  44%|████▍     | 22/50 [00:13<00:19,  1.40it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.274:  46%|████▌     | 23/50 [00:14<00:18,  1.49it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  48%|████▊     | 24/50 [00:15<00:16,  1.56it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.301:  50%|█████     | 25/50 [00:15<00:15,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  52%|█████▏    | 26/50 [00:16<00:14,  1.65it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.297:  54%|█████▍    | 27/50 [00:16<00:13,  1.67it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.293:  56%|█████▌    | 28/50 [00:17<00:13,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  58%|█████▊    | 29/50 [00:18<00:12,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.302:  60%|██████    | 30/50 [00:18<00:11,  1.69it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.297:  62%|██████▏   | 31/50 [00:19<00:15,  1.25it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.294:  64%|██████▍   | 32/50 [00:20<00:13,  1.34it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.308:  66%|██████▌   | 33/50 [00:21<00:11,  1.43it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.313:  68%|██████▊   | 34/50 [00:21<00:10,  1.50it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.311:  70%|███████   | 35/50 [00:22<00:09,  1.54it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.307:  72%|███████▏  | 36/50 [00:22<00:08,  1.57it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  74%|███████▍  | 37/50 [00:23<00:08,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.326:  76%|███████▌  | 38/50 [00:24<00:07,  1.60it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.327:  78%|███████▊  | 39/50 [00:24<00:06,  1.61it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.323:  80%|████████  | 40/50 [00:25<00:06,  1.62it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.320:  82%|████████▏ | 41/50 [00:26<00:07,  1.21it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.316:  84%|████████▍ | 42/50 [00:27<00:06,  1.30it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.313:  86%|████████▌ | 43/50 [00:27<00:05,  1.38it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.309:  88%|████████▊ | 44/50 [00:28<00:04,  1.44it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.304:  90%|█████████ | 45/50 [00:29<00:03,  1.48it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.302:  92%|█████████▏| 46/50 [00:29<00:02,  1.51it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.298:  94%|█████████▍| 47/50 [00:30<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.299:  96%|█████████▌| 48/50 [00:31<00:01,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.295:  98%|█████████▊| 49/50 [00:31<00:00,  1.53it/s]torch.Size([2, 3, 512, 512])
Test loss: 0.291: 100%|██████████| 50/50 [00:32<00:00,  1.54it/s]
(100, 512, 512)
Validation:
[Epoch: 19, numImages:   100]
Acc:0.8447095870971679, Acc_class:0.7507324723730173, mIoU:0.5265876065180695, fwIoU: 0.7442037977497376
Loss: 14.547
Recall/PDR:0.5865678663843802
Precision:0.11315801064856403
debug
CreateFile() Error: 2
CreateFile() Error: 2
CreateFile() Error: 2
CreateFile() Error: 2
CreateFile() Error: 2
CreateFile() Error: 2
CreateFile() Error: 2

Process finished with exit code 0
